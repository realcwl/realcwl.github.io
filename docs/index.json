[{"content":"Disclaimer: All of the following content are public information.\nAcademic Evals are Cracked It all starts with the question: How do you evaluate the quality of something that\u0026rsquo;s as strong as GPT-4 (or Gemini).\nBefore the era of large language model, various of researchers spend many times constructing evaluation benchmark to evaluate the model\u0026rsquo;s capability progress. I\u0026rsquo;d argue that a good benchmark is what drive progress in the field of NLP, and claiming the lead on a benchmark usually comes with fame and fortune, driving researchers and companies to compete with each other to create a better model.\nHowever, a lot of the benchmarks proposed in recent years are saturating in an unexpected speed. Take the following diagram as an example, it takes quite some time for a model to hill climb to human level performance in the first 15 years of this century, but just a few years (or months) for some of the newly proposed tasks such as GLUE.\nSaturating is just one side of things. Another very annoying issue is leakage. It\u0026rsquo;s actually quite easy to claim to the top of one leaderboard: You just train on the evaluation dataset. Such training can be intentional, we all know that there are enough dumb VCs who will just dumping money on you if you create a big headline on some leaderboard. But many times, the leakage is implicit and unintentional, with the massive training data, it\u0026rsquo;s quite common that you scraped some 3rd party website that happen to include a large chunk of MMLU data and by pretraining on it your model just remembers the answer. There are ways to detect such leakage, for example one way of detecting such leakage is by measuring the model\u0026rsquo;s perplexity on the eval set, and if model is more confident than some threshold we deem the eval set is leaked.\nAnother solution is to create benchmark that\u0026rsquo;s either hidden or dynamic. For example, ScaleAI has SEAL leaderboard on which they constantly conduct evaluation to measure the model\u0026rsquo;s capability. But as ScaleAI is also creating data for all these large language model providers, it\u0026rsquo;s not so convincing that they will actually adversarially exposing their clients model weakness. Another type of evaluation is to make it dynamic, with periodic refresh of the problem. LiveCodeBench is such a project where new problems are published constantly to ensure the freshness of data.\nHowever, even if we spend a lot of effort curating a very good dataset, we still face a quite serious problem: all of the aforementioned evaluations are quite artificial. A normal user doesn\u0026rsquo;t just go to LLM to ask leetcode questions, or asking a high school math selection problems. A user\u0026rsquo;s goal for the LLM is quite simple: ask real life questions and want a high quality answer. How can we make the evaluation focus on this front, while it\u0026rsquo;s quite challenging to effectively measure the open-ended answers?\nChatbot Arena in a nutshell Right after ChatGPT released, a few students in Berkeley Sky Computing Lab started to replicate ChatGPT\u0026rsquo;s success by creating an open source large language model called Vicuna. During the development of Vicuna, they started to realize it\u0026rsquo;s quite hard to comprehensively evaluate the large language model. To solve this issue, they began to develop an open platform that will later become Chatbot Arena, with a simple hypothesis: \u0026ldquo;human are good at judging quality of LLM if presented side by side\u0026rdquo;.\nWhen the user comes to the website, they can issue queries just like using any LLM provider. The query will be sent to 2 \u0026ldquo;random\u0026rdquo; LLMs to get responses. And then presented to the user for rating out of 4 options: 1) left is better. 2) right is better. 3) tie. 4) tie (both bad).\nAfter this rating steps. Arena will gather all the votes and rank the LLMs under comparison. How? Fortunately, human is a competitive animal, and we have already developed many ways to rank stuff. One of the most commonly used ranking systems is ELO ranking. Assuming 2 players in a zero sum game with ELO score \\(R_A\\) and \\(R_B\\). The probability of player \\(A\\) wins over player \\(B\\) is expressed as: $$ \\begin{equation} \\begin{aligned} P(A \u0026gt; B) \u0026amp;= \\frac{1}{1 + 10^{(R_B-R_A)/400}} \\end{aligned} \\end{equation} $$\nBy looking at all the comparisons submitted by users, we can derive one score for each model to best approximate the above probability. The following leaderboard is derived at 2024-08-11. As shown in Equation (1), the probability of Gemini-1.5-Pro-Exp-0801 being greater than GPT-4o-2024-05-13 is 51.87%1\nAs of the time of writing, this platform has collected comparisons across 127 different models, and a total votes of 1,610,507, and it has become the #1 trusted leaderboard for comparing and understand the strength of all the LLMs. There has been changes to this leaderboard such as introducing hard categories, multi-modal but the core idea is similar. That\u0026rsquo;s it about Chatbot Arena! A single open platform that open internet users can come and rate LLM on their questions, and a unified leaderboard to rank LLMs by their capabilities!\nYou don\u0026rsquo;t know Chatbot Arena What\u0026rsquo;s more to understand this leaderboard besides it\u0026rsquo;s a comparison platform and some magics to calculate the scores? I\u0026rsquo;d argue that there are many more details in the implementation of this leaderboard and can provide you insights on why some phenomenon exist. Most of the content below is my takeaway after reading their 2024 ICML paper.\nUnderstand the meaning of the score First, what does Arena score actually mean? Let\u0026rsquo;s take the leaderboard as an example. Gemini has an Elo score of 1299, while Claude 3.5 Sonnet has an Elo score of 1271. The actual meaning of these 2 score can be translated with equation (1) we derived above. That is,\n$$ \\begin{aligned} P(\\textbf{user prefer Gemini than Claude}) \u0026amp;= \\frac{1}{1 + 10^{(R_{claude}-R_{gemini})/400}} \\\\ \u0026amp;= \\frac{1}{1 + 10^{(1271-1299)/400}} \\\\ \u0026amp;= 0.54 \\end{aligned} $$\nIn another words, this is saying that on the queries coming from Arena users, Gemini wins Claude 3.5 54% of times.\nTo go into more details, we want to ask, how is this score calculated? A common misconception is that this Elo score is calculated based on online Elo ranking, same as how tennis/chess player's Elo is calculated in the equation below, where all the battles are sorted chronologically and one by one feed into the system for calculation. In the equation below, \\(S_A\\) is the actual result of the battle, while \\(E_A\\) is the predicted result from equation (1). $$ R^{\u0026rsquo;}_{A} = R_A + K \\cdot (S_A - E_A) $$\nChatbot Arena was initially using online Elo system for the ranking with a K-factor of 4, but online Elo system will prefer recent results more than the historical results. This will cause instability in the calculation, for example if you calculate the results twice with reverse ordering, the ranking will become quite different:\nInstead, Chatbot Arena today is using Maximum Likelihood Estimation with Bradley-Terry model to calculate the actual ranking. It reformulates the calculation of the final Elo score into the following MLE problem with logistic regression.\nSuppose we have \\(M\\) models under comparison, and there is no tie. Each model has an Elo estimation \\(E\\), the base of Elo is \\(k\\) (\\(k=10\\) as in the case of equation 1), and scale factor of \\(C\\) (\\(C=400\\) in equation (1)) and we have \\(T\\) observations of the outcome of battle, each denoted as a \\(M\\) length of vector \\(h\\) where the ith element has value $$ h_{t}^{i} = \\begin{cases} ln(k) \u0026amp; \\text{when ith model is the model a in battle t} \\\\ -ln(k) \u0026amp; \\text{when ith model is the model b in battle t} \\\\ 0 \u0026amp; \\text{when ith model doesn\u0026rsquo;t participate the battle t} \\end{cases} $$\nFinally, we let \\(y_{1...T}\\) be our target ground truth, where $$ y_{t} = \\begin{cases} 1 \u0026amp; \\text{when model a is the winner in battle t} \\\\ 0 \u0026amp; \\text{when model b is the winner in battle t} \\\\ \\end{cases} $$\nWe fit the following logistic regression model with no bias term and \\(M\\)-length model parameter \\(\\theta\\) $$ \\begin{equation} \\begin{aligned} \\hat{\\theta} \u0026amp;= \\argmin_\\theta{J(\\theta)} \\\\ \u0026amp;= \\argmin_{\\theta}-\\sum_{t=1}^{T} y_t * \\log{\\sigma{(h_t \\cdot \\theta)}} + (1 - y_t) * \\log{(1 - \\sigma{(h_t \\cdot \\theta)})} \\end{aligned} \\end{equation} $$\nIt's quite easy to prove that the logistic regression coefficient \\(\\hat{\\theta}\\) has a deterministic mapping to the optimal ranking Elo score where \\(E = C * \\hat{\\theta}\\). We give the rough intuition below. Suppose we only have a pair of model \\(i\\) and \\(j\\), out of all battles, we have \\(n\\) times model \\(i\\) wins and \\(m\\) times it loses. Thus the equation (2) becomes $$ \\begin{aligned} \\argmin_{\\theta} J \u0026amp;= -n * \\log{\\sigma{(ln(k) * \\theta_i - ln(k) * \\theta_j)}} - m * \\log{(1 - \\sigma{(ln(k) * \\theta_i - ln(k) * \\theta_j)})} \\\\ \u0026amp;= -\\frac{n}{n+m} * \\log{\\sigma{(ln(k) * \\theta_i - ln(k) * \\theta_j)}} - \\frac{m}{n+m} * \\log{(1 - \\sigma{(ln(k) * \\theta_i - ln(k) * \\theta_j)})} \\\\ \u0026amp;= -P_{\\text{i wins j}} * \\log\\Bigg[\\frac{1}{1 + \\exp^{-(ln(k) * \\theta_i - ln(k) * \\theta_j)}}\\Bigg] - (1 - P_{\\text{ wins j}}) * \\log\\Bigg[1 - \\frac{1}{1 + \\exp^{-(ln(k) * \\theta_i - ln(k) * \\theta_j)}}\\Bigg] \\\\ \u0026amp;= -P_{\\text{i wins j}} * \\log\\Bigg[\\frac{1}{1 + k^{\\theta_j - \\theta_i}}\\Bigg] - (1 - P_{\\text{ wins j}}) * \\log\\Bigg[1 - \\frac{1}{1 + k^{\\theta_j - \\theta_i}}\\Bigg] \\\\ \\end{aligned} $$\nlet \\(E = C * \\theta\\), we then have $$ \\begin{aligned} \\argmin_{\\theta} J \u0026amp;= -P_{\\text{i wins j}} * \\log\\Bigg[\\frac{1}{1 + k^{(E_j - E_i)/C}}\\Bigg] - (1 - P_{\\text{ wins j}}) * \\log\\Bigg[1 - \\frac{1}{1 + k^{(E_j - E_i)/C}}\\Bigg] \\\\ \u0026amp;= -P_{\\text{i wins j}} * \\log{\\hat{P}_{\\text{i wins j}}} - (1 - P_{\\text{i wins j}})* \\log{(1 - \\hat{P}_{\\text{i wins j}})} \\end{aligned} $$\nIt\u0026rsquo;s easy to see that the above equation is to calculate the cross-entropy between the Elo predicted win rate and the actual observed win rate. Thus, the best solution for equation (2) is the Elo score (or MLE coefficient) that predicts exactly the observed win rate.\nThis calculation is redone N times with bootstrapping to get confidence interval. N equals 100 in the case of the actual Arena implementation.\nSo what does all these calculation teaches us about the Elo score used in Chatbot Arena?\nFirst, because Bradley-Terry model is used, the model will treat the rating long time ago with the same weight as the it is recently. This might be okay for open source model where static weight is released. But for API based model, such evaluation can be problematic as previous rating is based on weak subversion than the latest most powerful release. That might be why companies like OpenAI and Google is using strange subversion names such as GPT-4o-2024-05-13 instead of just and umbrella version.\nSecondly, the score might hide the difference. The plot below is the win rate change as we change the Elo delta.\nAs can be seen here, even a 100 Elo delta renders the win rate from 50% to 65%. When you think about the model difference, I think it\u0026rsquo;s quite helpful to put it into the context of the actual win rate.\nLook Into Different Slice When you look at the overall slice, the number might suggest that a lot of models are similar. The following graph is a good example of this, this is a snapshot taken earlier this year (not as of time of writing). While it may seems that claude-3-opus is performing worse than llama-3-70b-instruct, when you limit to hard prompt, the performance clearly shows another picture. While the Elo rating is based on the overall uniform sampling of all the ratings, we as user almost always care more about the more challenging prompts. In the end, it doesn\u0026rsquo;t matter how well your model answers \u0026ldquo;how are you\u0026rdquo;, but instead how your model answers \u0026ldquo;help me translate this C++ program into Haskell\u0026rdquo;.\nI highly suggest anyone, when picking a model, take a look at the segment that you care about. I suggest definitely take a look at hard and coding, math as these segments correlates with the model\u0026rsquo;s reasoning capability strongly. For example, while ranked #1 in the overall segment, Gemini-1.5-Pro-Exp-0801\u0026rsquo;s coding capability significantly lags behind competitors, be careful when you choose it for your coding task.\nSummary I hope this articles provided you more detailed mechanisms for Chatbot Arena. Overall, it\u0026rsquo;s an open source platform where user across the world assign ratings to chatbot, and one leaderboard that ranks the model\u0026rsquo;s capability. This simple idea of using communities to help evaluate open-ended prompts turned out to be quite brilliant. But it is not without limitation. The most important limitation is bias, the users that actually coming to this website are majority software engineers and thus shifted this leaderboard heavily towards software engineering. Also the trustworthiness of the rater can be questionable sometimes, although the platform used many mechanisms to detect anormalous users, if you look at some of the safety ratings from the released Arena 33k, clearly the user prefers unsafe content even though the model should moderate sometimes.\nBut overall, I want you to appreciate the beauty of open source community and the power of simple ideas. Chatbot Arena has become the #1 leaderboard in the LLM industry. I hope this post helps you understand it a bit better!\n51.87% = 1/(1 + 10**((1286-1299)/400))\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://realcwl.github.io/posts/you_dont_know_lmsys/","summary":"Disclaimer: All of the following content are public information.\nAcademic Evals are Cracked It all starts with the question: How do you evaluate the quality of something that\u0026rsquo;s as strong as GPT-4 (or Gemini).\nBefore the era of large language model, various of researchers spend many times constructing evaluation benchmark to evaluate the model\u0026rsquo;s capability progress. I\u0026rsquo;d argue that a good benchmark is what drive progress in the field of NLP, and claiming the lead on a benchmark usually comes with fame and fortune, driving researchers and companies to compete with each other to create a better model.","title":"Chatbot Arena in a nutshell"},{"content":"It\u0026rsquo;s well known that the state-of-the-art LLM models are trained with massive human quality feedback. This feedback is either coming from a massive rater pool, or can come from the end users implicitly (sometimes explicitly as well.. remember when ChatGPT presents you 2 responses to choose from?).\nHowever, I found that there are many subtleties for one to truly understand what\u0026rsquo;s going on under the hood. This is a blog to capture my understanding of the RLHF algorithm, and how it evolves into rewardless model such as DPO and IPO.\nA quick recap of RLHF Suppose our reward is point-wise, meaning we get a single float number to denote the quality of the response based on the context, we try to optimize for the following objective in RLHF\n$$ \\begin{equation} \\begin{aligned} J(\\pi) \u0026amp;= \\mathbb{E}_{\\substack{x \\sim D \\\\ y \\sim \\pi(\\cdot|x)}} \\left[ r(x, y) \\right] - \\beta\\mathbb{D}_\\text{KL}(\\pi ||\\pi_\\text{ref}) \\end{aligned} \\end{equation} $$\nThe reward function \\( r(x, y) \\) usually isn't directly differentiable in relative to \\(\\pi\\) itself (for example, in auto-regressive model where we sample each token with argmax). Thus the most common approach to deal with this case is to use actor critic method to find the best policy. Optimizing equation (1) usually consists of the following general steps, assuming we are using PPO\n1. Training the reward model\nSuppose we have access of many \\( (\\text{prompt}, \\text{likert score}) \\) pairs, we can train the reward model \\( r(x, y) \\) with MSE loss and with a neural network. One good thing is that such neural network can be directly initialized with the underlying policy transformer network, and replace the output layers to be a shallow FFN. 2. Sample the policy, and estimate value\nDuring this step, suppose we have access to many prompts that matches the distribution we want to optimize, say \\( x \\sim D \\), we can thus run the policy \\(T\\) times and send the responses for reward scoring. This will results a distribution \\( \\text{M} \\) with \\(T\\) episodes where each episode \\( \\text{M}^t \\) consists of: $$ \\text{M}^t = (\\text{prompt x}^t, \\text{response y}^t, \\text{score r}^t, \\text{value v}^t, \\text{probability p}^t) $$\nAdditionally, we also generate 2 more things:\nvalue: This is generated by a value network, which is needed to stabilize the RL training to make it offset invariant. See PPO paper for details. One good thing about it is that such value network can be initialized by the policy SFT\u0026rsquo;ed network too, similar to our reward model. probability: This is the probability of the response sequence (of length n), equals to $$ \\prod_{i=1}^{n} p(y_i \\mid y_1, y_2, \\ldots, y_{i-1}, x) $$ this will later be used to calculate the KL divergence. 3. Train with PPO\nFollowing the common PPO training, we have the loss function across all the \\(T\\) episodes as $$ \\begin{aligned} L^{\\text{PPO}}(\\pi) = \\mathbb{E}_{t \\sim M} \\Bigg[ \u0026amp; \\min \\left( \\rho_t(\\pi) \\hat{A}_t, \\text{clip} \\left( \\rho_t(\\pi), 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{A}_t \\right) \\Bigg] \\end{aligned} $$\nwhere, $$ \\begin{aligned} \\rho_t(\\pi) \u0026amp;= \\frac{\\pi(y^t | x^t)}{\\pi_{\\text{ref}}(y^t | x^t)} \\\\ \\hat{A}_t \u0026amp;= J_t - v^t \\\\ J_t \u0026amp;= r^t - \\beta\\mathbb{D}_{\\text{KL}}(\\pi || \\pi_{\\text{ref}}) \\end{aligned} $$\nI think it\u0026rsquo;s worth pointing out the calculation of the KL divergence (at least it took me a long time to figure out). The definition of KL divergence is\n$$ D_{\\text{KL}}(\\pi \\parallel \\pi_{\\text{ref}}) = \\sum_{y} \\pi(y|x) \\log \\left( \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} \\right) $$\ncomprehensively evaluate for all y is computationally intractable (like, you need to exhaust all possible human sentences). But recall that the meaning of KL divergence is measure how different 2 distribution is, and that can also be translated to how different 2 distribution assign probability to the same sequence sample from one of them1.\nThus a noisy estimation of the KL divergence can be expressed as\n$$ D_{\\text{KL}}(\\pi \\parallel \\pi_{\\text{ref}}) = \\mathbb{E}_{x \\sim M}[\\text{KL}(\\pi(\\cdot|x) || \\pi_{\\text{ref}}(\\cdot|x))] $$\nwhere\n$$ \\mathbb{E}_{x \\sim M}[\\text{KL}(\\pi(\\cdot|x) || \\pi_{\\text{ref}}(\\cdot|x))] = \\mathbb{E}_{y \\sim \\pi(\\cdot|x)}\\Bigg[\\log(\\frac{\\pi(y|x)}{\\pi_{\\text{ref}(y|x)}}) \\Bigg] $$\nYou can also check this implementation to gain a deeper understanding of it.\n4. Update value estimator\nFinally, once the model has been updated, the value function is fitting to the actual reward. You repeat 2-4 until the model is aligned with the preference.\nDeal with pairwise preference The previous section is assuming a point wise preference labeling, that is, each reward labeling is of pair $$ (\\text{prompt}, \\text{likert score}) $$ However, human is not very good at rating absolutely, but very good as comparing things side by side. For example, it\u0026rsquo;s hard to say how hot the weather is exactly in terms of Celsius, but it\u0026rsquo;s quite easy to tell the the weather in the evening is more chill compared to the afternoon. The same intuition applies for LLM training, while it\u0026rsquo;s hard to collect absolute likert score, it\u0026rsquo;s relatively easy to collect a triplet of $$ (\\text{prompt}, \\text{response}_w, \\text{response}_l) $$\nWhere the 2 responses are 2 samples and we collect the preference score from a rater pool about which one is better (\\(\\text{response}_w\\)) and which one is worse (\\(\\text{response}_l\\)) in the context of the prompt. Rather than training the reward model to fit the absolute score, the reward model instead trained to fit a Bradley-Terry model 2.\nLet the prompt be \\(x\\) and \\( \\text{response}_w, \\text{response}_l \\) be \\(y_w, y_l\\) respectively. The objective instead becomes $$ L(r) = -\\mathbb{E}_{(x, y_w, y_l)}[\\log(p(y_w\u0026gt;y_l|x))] $$ where $$ p(y_w\u0026gt;y_l|x) = \\sigma(r(x,y_w) - r(x, y_l)) $$\nSee this implementation to get a deeper understanding of the pairwise reward model training. Once the reward model is trained, we use PPO with the same objective as in equation (1) to perform RLHF.\nDPO: Avoid the reinforcement learning at all Let\u0026rsquo;s recap, in RLHF (with PPO), we need to bring up 3 models to do different things. We need our policy model to create episodes and update, a reward model to score the generated response and a value model to estimate the value in order to stabilize the training. Although theoretically a lot of parameters can be shared across them 3, it is still relatively resource intensive compared to instruction fine tuning. In addition, it requires an additional step to train the reward model, which leaves more room for error.\nThus, we are interested in a reward free method which can be more resource/time efficient method that can avoid the above issue. And guess what, DPO does exact that with a quite beautiful mathematic trick.\nRecall that our objective, even though non differentiable, is $$ \\begin{aligned} J(\\pi) \u0026amp;= \\mathbb{E}_{\\substack{x \\sim D \\\\ y \\sim \\pi(\\cdot|x)}} \\left[ r(x, y) \\right] - \\beta\\mathbb{D}_\\text{KL}(\\pi ||\\pi_\\text{ref}) \\end{aligned} $$ Let\u0026rsquo;s see if we can have an analytic solution to the optimal policy $$ \\begin{aligned} \\max_\\pi J \u0026amp;= \\max_\\pi\\mathbb{E}_{\\substack{x \\sim D \\\\ y \\sim \\pi(\\cdot|x)}} \\left[ r(x, y) \\right] - \\beta\\mathbb{D}_\\text{KL}(\\pi ||\\pi_\\text{ref}) \\\\ \u0026amp;= \\max_\\pi\\mathbb{E}_{\\substack{x \\sim D \\\\ y \\sim \\pi(\\cdot|x)}} \\left[ r(x, y) \\right] - \\beta\\mathbb{E}_{\\substack{x \\sim D \\\\ y \\sim \\pi(\\cdot|x)}}\\Bigg[\\log(\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)})\\Bigg] \\\\ \u0026amp;= \\max_\\pi\\mathbb{E}_{\\substack{x \\sim D \\\\ y \\sim \\pi(\\cdot|x)}} \\Bigg[ r(x, y) - \\beta\\log(\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)})\\Bigg] \\\\ \u0026amp;= \\min_\\pi\\mathbb{E}_{\\substack{x \\sim D \\\\ y \\sim \\pi(\\cdot|x)}} \\Bigg[ \\beta\\log(\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}) - r(x, y) \\Bigg] \\\\ \u0026amp;= \\min_\\pi\\mathbb{E}_{\\substack{x \\sim D \\\\ y \\sim \\pi(\\cdot|x)}} \\Bigg[ \\beta\\log(\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\cdot\\exp(\\frac{1}{\\beta}r(x, y))}) \\Bigg] \\\\ \u0026amp;= \\beta\\min_\\pi\\mathbb{E}_{\\substack{x \\sim D \\\\ y \\sim \\pi(\\cdot|x)}} \\Bigg[ \\log{\\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\cdot\\exp(\\frac{1}{\\beta}r(x, y))}} - \\log{Z(x)}\\Bigg] \\\\ \\end{aligned} $$\nlet \\(Z(x) = \\sum_{\\text{all of } y \\sim \\pi(\\cdot|x)}{\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x, y))} \\), we can see that the denominator in the above equation is a valid distribution (probability adds up to 1). Thus, the objective function becomes $$ \\begin{equation} \\begin{aligned} \\max_\\pi J \u0026= \\beta\\min_\\pi\\mathbb{E}_{x \\sim D } [\\mathbb{D}_\\text{KL}(\\pi(y|x) ||\\pi_*(y|x)) - \\log{Z(x)}] \\end{aligned} \\end{equation} $$ where $$ \\begin{equation} \\pi_*(y|x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\cdot\\exp(\\frac{1}{\\beta}r(x, y)) \\end{equation} $$ because \\(Z(x)\\) is exactly irrelevant to \\(\\pi\\), we know that solution to equation (2) is \\(\\pi = \\pi_*\\). Thus equation (3) is our optimal policy model that perfectly aligns with the reward model. Up until know, our conclusion in (3) doesn't help so much. That's because we have a nasty term \\(Z(x)\\) which requires up to calculate the probability of all possible y given x which is impractical in practice. But here comes the fun part: With some mathematic tricks, we can cancel out the incomputable Z and get a computable loss function!:\nTo see how, let's assume our reward model \\(r^*\\) fits the human preference with a Bradley-Terry model. That is, we have $$ \\begin{equation} p(y_w\u003ey_l|x) = \\sigma(r(x, y_w) - r(x, y_l)) \\end{equation} $$ Because we have \\(\\pi_*(y|x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\cdot\\exp(\\frac{1}{\\beta}r(x, y))\\), we can reparametrize the optimal reward as $$ \\begin{equation} r(x, y) = \\beta\\log{\\frac{\\pi^*(y|x)}{\\pi_{\\text{ref}}(y|x)}} + \\beta\\log{Z(x)} \\end{equation} $$ If we substitute (5) into (4), surprisingly we canceled out the term Z! And the Bradley-Terry model can be expressed as $$ \\begin{equation} p(y_w\u0026gt;y_l|x) = \\sigma\\Bigg(\\beta\\log{\\frac{\\pi^*(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}} - \\beta\\log{\\frac{\\pi^{\\ast}(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}}\\Bigg) \\end{equation} $$\nI just want to stop here and appreciate the beauty of the above derivation. Let\u0026rsquo;s think about what this final equation (6) actually implies:\nFinding the perfect reward model that fits the pair wise preference in (4) is mathematically equivalent to finding a perfect policy model that fits the same pair wise preference. Any reward model has a corresponding policy model that captures the same learned preference (equation (3)) Thus, our final objective becomes $$ \\begin{equation} L_{\\text{DPO}}(\\pi;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l) \\sim D}\\Bigg[ \\log{\\sigma\\Bigg(\\beta\\log{\\frac{\\pi^*(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}} - \\beta\\log{\\frac{\\pi^{\\ast}(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}}\\Bigg)} \\Bigg] \\end{equation} $$\nNote that the above objective (7) is differentiable, and thus we don\u0026rsquo;t need reinforcement learning to learn the objective, but instead can just train the policy model with back propagation! This thus greatly simplifies our training process.\nWe suggest you to also look at the original DPO paper where it analyze the property of its gradient to build more intuition. But to sum up, the policy model together with the original reference model (SFT model) encodes implicitly the preference in the reward model training dataset. Such preference doesn\u0026rsquo;t require training a RM as a proxy but instead can directly train the policy model to learn. Due to such simplicity, in fact most of the open source large language model use some version of DPO to train the model.\nLimitation of DPO But is DPO the perfect method to train LLM? Why we never heard that OpenAI is using DPO to perform GPT post-training?\nMany works456 analyze the problem, and shows that DPO has the following theoretical limitations:\nDPO will assign high value to out-of-distribution samples 4 Suppose we have only 3 actions listed in the table below\nAction y1 y2 y3 πref 0.5 0.5 0 Dpref {(yw = y1, yl = y2)} πDPO 0.1 0.0 0.9 πPPO 1 0 0 We can see that DPO can learn the solution of 0.1, 0, 0.9. This is because the reward function\n$$ \\begin{aligned} L_{\\text{DPO}}(\\pi;\\pi_{\\text{ref}})\u0026amp; =-\\mathbb{E}_{(x,y_w,y_l) \\sim D}\\Bigg[ \\log{\\sigma\\Bigg(\\beta\\log{\\frac{\\pi^*(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}} - \\beta\\log{\\frac{\\pi^{\\ast}(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}}\\Bigg)} \\Bigg] \\\\ \u0026amp;= -\\log{\\sigma\\Bigg(\\beta(\\log(\\frac{a}{0.5})-\\log{\\frac{b}{0.5}})\\Bigg)} \\\\ \u0026amp;= \\log\\Bigg(1+(\\frac{a}{b})^\\beta\\Bigg) \\end{aligned} $$\nThus as long as \\(a=0\\), it is an optimal policy from the view of DPO. However, such policy won't be an optimal one for PPO training, as the KL divergence will ensure the model to stay close to the reference model. This is also discovered in the Nvidia Nemotron5 training, where in the tech report, they reported that\nWhile the policy learns to differentiate chosen and rejected responses, we observe both chosen and rejected responses’ likelihoods drop consistently with their gap increasing, even if chosen responses are high-quality.\nIn addition, RLHF with PPO can leverage prompt only date, and such dataset act as a good regularizer of the model training to avoid it bias toward OOD samples.\nDPO will overfit when the preference is too deterministic In DeepMind's analysis, under the assumption that \\(p^\\ast\\) fits a Bradley-Terry model, we can prove that for the learning objective below $$ \\max_\\pi\\mathbb{E}_{\\substack{x \\sim D \\\\ y \\sim \\pi(\\cdot|x) \\\\ y\u0026rsquo; \\sim \\mu(\\cdot|x)}}[\\Psi(p^\\ast(y\u0026gt;y\u0026rsquo;|x))] - \\tau\\mathbb{D}_{\\text{KL}}(\\pi || \\pi_{\\text{ref}}) $$\nit is exactly the same as the RLHF objective in (1), and thus equivalent to the DPO learning objective in (7), when $$ \\Psi(q) = \\log(\\frac{q}{1-q}) $$\nIt's actually quite easy to prove this proposition and we leave that to the reader. But looking at the shape of this function, we can see that this function has a very large gradient near the point of 0 and 1. When training with this objective, a small increase in the preference near 0 or 1 will just be as incentivized as a large increase near 0.5. Thus, when the ground truth label is very deterministic (such as machine based feedback), thus no matter how large the regularization term \\(\\tau\\) is, the model will eventually deviate away from the reference and thus overfit drastically. This is also demonstrated in the experiment of IPO, where no matter how large the regularizer is, the policy will eventually deviate away from the original one. However, this won\u0026rsquo;t happen in RLHF not only because a RM is trained to proxy the raw preference and serves as a regularizer, it utilize PPO which only moves the policy in a trust region. This might be why DeepSeek V2 Coder is using an RM to regularize the execution feedback instead of sending it directly for learning.\nSummary Okay, you made it to the end, congrats :) Before you go, let\u0026rsquo;s just recap what we have learned:\nRLHF with PPO is the standard way of aligning the post SFT model towards annotated human preference. Pairwise preference is fitted with a Bradley-Terry model. DPO utilize a reparametrization trick, and prove that theoretically the policy model captures the preference and can be directly learned via back propagation. While DPO is simpler and more computational efficient, it suffers from regularization problem for both OOD and deterministic feedback (such as 0/1), thus while it generally performs well in more subjective tasks where a multi-scale likert score is used, it should be used with caution when the feedback is too deterministic or preference dataset is too small. I hope you like this post and please don\u0026rsquo;t hesitate to give your suggestion (scroll all the way up and click \u0026ldquo;Suggest Changes\u0026rdquo;). Thanks!\nSee this video for a very good explanation with coin toss.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://web.stanford.edu/class/archive/stats/stats200/stats200.1172/Lecture24.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGoogle Sparrow: https://arxiv.org/abs/2209.14375\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDPO will increase the likelihood of sample that\u0026rsquo;s OOD of preference pairs https://arxiv.org/abs/2404.10719\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNvidia NemoTron Tech Report: https://arxiv.org/html/2406.11704v1\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGoogle IPO: https://arxiv.org/abs/2310.12036\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://realcwl.github.io/posts/rlhf_to_ipo/","summary":"It\u0026rsquo;s well known that the state-of-the-art LLM models are trained with massive human quality feedback. This feedback is either coming from a massive rater pool, or can come from the end users implicitly (sometimes explicitly as well.. remember when ChatGPT presents you 2 responses to choose from?).\nHowever, I found that there are many subtleties for one to truly understand what\u0026rsquo;s going on under the hood. This is a blog to capture my understanding of the RLHF algorithm, and how it evolves into rewardless model such as DPO and IPO.","title":"From RLHF to Direct Preference Learning"},{"content":"The last 6 months have been a challenging and tough period in my life, prompting considerable reading and self-reflection. Now as things are finally getting back on track, I\u0026rsquo;d like to share some valuable lessons that helped me. If you find yourself sitting in front of the computer, bored enough to open this article, you might find these lessons helpful to you as well.\nEmbrace Longtermism The concept of longtermism embodies various layers of interpretation. It might seem outdated or cliche in a fast-paced world where technological advancements progress rapidly. However, I maintain that focusing on long-term gains, while disregarding short-term fluctuations, is not only worthwhile but increasingly essential.\nGood capital theory Good capital is ruthless for profit, and patient for scaling.\nI first encoutered this great quote in one of my favorite author Clayton Christensen\u0026rsquo;s book 1. The dichotomy of short term and long term effort, and the dynamic relathionship is greatly summarized in this sentence. Specifically, the longtermism I\u0026rsquo;m referring to is exactly the one mentioned in this quote.\nA mindset of \u0026ldquo;fail fast\u0026rdquo; should always be embraced, which helps you navigates the uncertainties in today\u0026rsquo;s technological shift, this is the \u0026ldquo;ruthless for profit\u0026rdquo; part. However, I think the true difficulty lies in the patiently scaling side, where one can get distracted easily.\nI often find myself thinking \u0026ldquo;Auh.. I wish I had spent the last year solidify this skill\u0026rdquo;, whether it be exercising, learning a new language, mathmatic skills or writing. Like many others, I often find myself too eager to reap the benefits before properly sowing the seeds. In reality, life is a marathon. Once you know you are heading towards the right direction, it\u0026rsquo;s crucial to grow the necessary skills before scale it up.\nGrit, your effort counts twice $$ \\text{achivements} = \\text{gift} * \\text{effort}^2 $$\nThis is another great summary I read from the famous book \u0026ldquo;Grit\u0026rdquo;, and also something I deeply felt in my past experience. While I think I\u0026rsquo;m a quick learner that tend to grasp 80% of things in a short amount of time, I some times failed to spend enough efforts to achieve mastery. The key idea encoded in this equation is that, while you can achieve a skill with high gift and low effort. True achievements can only be achieved when you also spend enough time practicing those skills and become an expert.\nI also find this quote optimistic. When you work hard, and much harder than others, the quadratic multiplier will eventually catch up and let you surpass anyone who is either gifted or privileged.\nPatience For those who aim high, It seems many are eager to achieve success quickly. I even have friends who made hundreds of millions of dollars in their mid 20s.\nBut those are indeed exceptionally rare cases. Remember your life is a marathon, and you should measure your life in a span of years. Try to compare to who you were yesterday, reflect, and focus on doing better today. If I could tell myself something 5 years ago, I\u0026rsquo;d say \u0026ldquo;give it more time\u0026rdquo;.\nI\u0026rsquo;ve also come to observe that the astonishing achievements don\u0026rsquo;t just appear out of thin air. They are usually the result of years of practice, failure and growth, all accumulating to a significant turning point. Therefore, one should not expect overnight success without years of dedication.\nMoney is only a means to an end In my first few years, I was really desparate to achieve financial security. The finantial burden on my family is quite real.\nHowever, just a few years after graduation (thanks to Google), I managed to achieve that financial independence. Gradually, I realized that money isn\u0026rsquo;t on top of my list.\nI won\u0026rsquo;t deny that it feels great to get a pay raise, or a huge bonus. But even when I do get money, spending it on others - like host a party with my best friends, buying gifts for my parent - brings me more joy than spending them on myself.\nMy true joy really comes from working on very important things and solving hard challenges whose solutions are beneficial to the society, and with people I deeply love and respect. When I\u0026rsquo;m doing so, the fulfillment from the process is as much as the reward from the outcome. In that sense, I\u0026rsquo;m very happy right now to tackle some of the hardest problems with my friends in the new Labs team.\nOne last thing\u0026hellip; your action defines your value As the final part, I think it worth talk about the word value.\nOver the past 6 months, I worked in a team where a winning strategy was clear but unsettling: high achievers in this team require aligning with your boss\u0026rsquo;s desire, double down on it and ruthlessly micromanage anyone assigned to the task (Note, you don\u0026rsquo;t have to be technically strong scientifically correct, it\u0026rsquo;s all about people games). In this environment, other values like accountability, transparency, creativity, or even basic respect were disregarded.\nSome of the middle level managers tried to justify their actions by saying \u0026ldquo;oh I\u0026rsquo;m in a tough spot\u0026rdquo;. However, I truly believe it\u0026rsquo;s your action that justify your value, not what you talk about.\nJean-Paul Sartre once said, \u0026ldquo;In fashioning myself I fashion man2\u0026rdquo; suggesting that every choice we make sets a moral standard for others. That applies here, it\u0026rsquo;s true that those middle managers can justify their actions (or inaction) with the pretext that it\u0026rsquo;s just temporary. Yet fundamentally, they chose a path where climbing Google\u0026rsquo;s corporate ladder takes precedence over building an impactful product, or being a good person. Essentially, they chose their values through actions.\nMy takeaway? You should act what you value, and put your moral compass before subordinate goals such as money or power.\nI truly believe that everyone bears the responsibility to shape the world with their values, and it is your action in daily life does that. It is crucial to act in accordance with your beliefs to lead a coherent life. Admittedly, doing so will be extremely challenging at times. But in doing so, I believe, is an essential expression of freedom and independent spirit.\nHow will you measure your life\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis is from Jean-Paul Sartre\u0026rsquo;s famous Existentialism Is a Humanism, in which he expound how your choice defines you. A must read that I highly recommend.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://realcwl.github.io/posts/lesson_i_learned/","summary":"The last 6 months have been a challenging and tough period in my life, prompting considerable reading and self-reflection. Now as things are finally getting back on track, I\u0026rsquo;d like to share some valuable lessons that helped me. If you find yourself sitting in front of the computer, bored enough to open this article, you might find these lessons helpful to you as well.\nEmbrace Longtermism The concept of longtermism embodies various layers of interpretation.","title":"Some lessons I learned the hard way"}]