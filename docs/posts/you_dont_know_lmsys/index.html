<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>You don&#39;t know Chatbot Arena | Weilun&#39;s Homepage</title>
<meta name="keywords" content="LLM, Evaluation">
<meta name="description" content="Let&#39;s get a bit nerdy on the most trusted LLM leaderboard">
<meta name="author" content="Weilun Chen">
<link rel="canonical" href="https://realcwl.github.io/posts/you_dont_know_lmsys/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5ff2630c4d1b3e25bc21f0ecd96681dbcf58219e741fa627857820b5485cb770.css" integrity="sha256-X/JjDE0bPiW8IfDs2WaB289YIZ50H6YnhXggtUhct3A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://realcwl.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://realcwl.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://realcwl.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://realcwl.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://realcwl.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://realcwl.github.io/posts/you_dont_know_lmsys/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>



<script async src="https://www.googletagmanager.com/gtag/js?id=G-WR64WXTW6H"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-WR64WXTW6H');
</script>
  

<meta property="og:title" content="You don&#39;t know Chatbot Arena" />
<meta property="og:description" content="Let&#39;s get a bit nerdy on the most trusted LLM leaderboard" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://realcwl.github.io/posts/you_dont_know_lmsys/" />
<meta property="og:image" content="https://realcwl.github.io/images/cover.jpeg" />
<meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-08-07T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-08-07T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://realcwl.github.io/images/cover.jpeg" />
<meta name="twitter:title" content="You don&#39;t know Chatbot Arena"/>
<meta name="twitter:description" content="Let&#39;s get a bit nerdy on the most trusted LLM leaderboard"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://realcwl.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "You don't know Chatbot Arena",
      "item": "https://realcwl.github.io/posts/you_dont_know_lmsys/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "You don't know Chatbot Arena",
  "name": "You don\u0027t know Chatbot Arena",
  "description": "Let's get a bit nerdy on the most trusted LLM leaderboard",
  "keywords": [
    "LLM", "Evaluation"
  ],
  "articleBody": "Disclaimer: All of the following content are public information.\nLLM evals are broken It all starts with the question: How do you evaluate the quality of something that’s as strong as GPT-4 (or Gemini).\nBefore the era of large language model, various of researchers spend many times constructing evaluation benchmark to evaluate the model’s capability progress. I’d argue that a good benchmark is what drive progress in the field of NLP, and claiming the lead on a benchmark usually comes with fame and fortune, driving researchers and companies to compete with each other to create a better model.\nHowever, a lot of the benchmarks proposed in recent years are saturating in an unexpected speed. Take the following diagram as an example, it takes quite some time for a model to hill climb to human level performance in the first 15 years of this century, but just a few years (or months) for some of the newly proposed tasks such as GLUE.\nSaturating is just one side of things. Another very annoying issue is leakage. It’s actually quite easy to claim to the top of one leaderboard: You just train on the evaluation dataset. Such training can be intentional, we all know that there are enough dumb VCs who will just dumping money on you if you create a big headline on some leaderboard. But many times, the leakage is implicit and unintentional, with the massive training data, it’s quite common that you scraped some 3rd party website that happen to include a large chunk of MMLU data and by pretraining on it your model just remembers the answer. There are ways to detect such leakage, for example one way of detecting such leakage is by measuring the model’s perplexity on the eval set, and if model is more confident than some threshold we deem the eval set is leaked.\nAnother solution is to create benchmark that’s either hidden or dynamic. For example, ScaleAI has SEAL leaderboard on which they constantly conduct evaluation to measure the model’s capability. But as ScaleAI is also creating data for all these large language model providers, it’s not so convincing that they will actually adversarially exposing their clients model weakness. Another type of evaluation is to make it dynamic, with periodic refresh of the problem. LiveCodeBench is such a project where new problems are published constantly to ensure the freshness of data.\nHowever, even if we spend a lot of effort curating a very good dataset, we still face a quite serious problem: all of the aforementioned evaluations are quite artificial. A normal user doesn’t just go to LLM to ask leetcode questions, or asking a high school math selection problems. A user’s goal for the LLM is quite simple: ask real life questions and want a high quality answer. How can we make the evaluation focus on this front, while it’s quite challenging to effectively measure the open-ended answers?\nChatbot Arena in a nutshell Right after ChatGPT released, a few students in Berkeley Sky Computing Lab started to replicate ChatGPT’s success by creating an open source large language model called Vicuna. During the development of Vicuna, they started to realize it’s quite hard to comprehensively evaluate the large language model. To solve this issue, they began to develop an open platform that will later become Chatbot Arena, with a simple hypothesis: “human are good at judging quality of LLM if presented side by side”.\nWhen the user comes to the website, they can issue queries just like using any LLM provider. The query will be sent to 2 “random” LLMs to get responses. And then presented to the user for rating out of 4 options: 1) left is better. 2) right is better. 3) tie. 4) tie (both bad).\nAfter this rating steps. Arena will gather all the votes and rank the LLMs under comparison. How? Fortunately, human is a competitive animal, and we have already developed many ways to rank stuff. One of the most commonly used ranking systems is ELO ranking. Assuming 2 players in a zero sum game with ELO score \\(R_A\\) and \\(R_B\\). The probability of player \\(A\\) wins over player \\(B\\) is expressed as: $$ \\begin{equation} \\begin{aligned} P(A \u003e B) \u0026= \\frac{1}{1 + 10^{(R_B-R_A)/400}} \\end{aligned} \\end{equation} $$\nBy looking at all the comparisons submitted by users, we can derive one score for each model to best approximate the above probability. The following leaderboard is derived at 2024-08-11. As shown in Equation (1), the probability of Gemini-1.5-Pro-Exp-0801 being greater than GPT-4o-2024-05-13 is 51.87%1\nAs of the time of writing, this platform has collected comparisons across 127 different models, and a total votes of 1,610,507, and it has become the #1 trusted leaderboard for comparing and understand the strength of all the LLMs. There has been changes to this leaderboard such as introducing hard categories, multi-modal but the core idea is similar. That’s it about Chatbot Arena! A single open platform that open internet users can come and rate LLM on their questions, and a unified leaderboard to rank LLMs by their capabilities!\nYou don’t know Chatbot Arena What’s more to understand this leaderboard besides it’s a comparison platform and some magics to calculate the scores? I’d argue that there are many more details in the implementation of this leaderboard and can provide you insights on why some phenomenon exist. Most of the content below is my takeaway after reading their 2024 ICML paper.\nUnderstand the meaning of the score First, what does Arena score actually mean? Let’s take the leaderboard as an example. Gemini has an Elo score of 1299, while Claude 3.5 Sonnet has an Elo score of 1271. The actual meaning of these 2 score can be translated with equation (1) we derived above. That is,\n$$ \\begin{aligned} P(\\textbf{user prefer Gemini than Claude}) \u0026= \\frac{1}{1 + 10^{(R_{claude}-R_{gemini})/400}} \\\\ \u0026= \\frac{1}{1 + 10^{(1271-1299)/400}} \\\\ \u0026= 0.54 \\end{aligned} $$\nIn another words, this is saying that on the queries coming from Arena users, Gemini wins Claude 3.5 54% of times.\nTo go into more details, we want to ask, how is this score calculated? A common misconception is that this Elo score is calculated based on online Elo ranking, same as how tennis/chess player's Elo is calculated in the equation below, where all the battles are sorted chronologically and one by one feed into the system for calculation. In the equation below, \\(S_A\\) is the actual result of the battle, while \\(E_A\\) is the predicted result from equation (1). $$ R^{’}_{A} = R_A + K \\cdot (S_A - E_A) $$\nChatbot Arena was initially using online Elo system for the ranking with a K-factor of 4, but online Elo system will prefer recent results more than the historical results. This will cause instability in the calculation, for example if you calculate the results twice with reverse ordering, the ranking will become quite different:\nInstead, Chatbot Arena today is using Maximum Likelihood Estimation with Bradley-Terry model to calculate the actual ranking. It reformulates the calculation of the final Elo score into the following MLE problem with logistic regression.\nSuppose we have \\(M\\) models under comparison, and there is no tie. Each model has an Elo estimation \\(E\\), the base of Elo is \\(k\\) (\\(k=10\\) as in the case of equation 1), and scale factor of \\(C\\) (\\(C=400\\) in equation (1)) and we have \\(T\\) observations of the outcome of battle, each denoted as a \\(M\\) length of vector \\(h\\) where the ith element has value $$ h_{t}^{i} = \\begin{cases} ln(k) \u0026 \\text{when ith model is the model a in battle t} \\\\ -ln(k) \u0026 \\text{when ith model is the model b in battle t} \\\\ 0 \u0026 \\text{when ith model doesn’t participate the battle t} \\end{cases} $$\nFinally, we let \\(y_{1...T}\\) be our target ground truth, where $$ y_{t} = \\begin{cases} 1 \u0026 \\text{when model a is the winner in battle t} \\\\ 0 \u0026 \\text{when model b is the winner in battle t} \\\\ \\end{cases} $$\nWe fit the following logistic regression model with no bias term and \\(M\\)-length model parameter \\(\\theta\\) $$ \\begin{equation} \\begin{aligned} \\hat{\\theta} \u0026= \\argmin_\\theta{J(\\theta)} \\\\ \u0026= \\argmin_{\\theta}-\\sum_{t=1}^{T} y_t * \\log{\\sigma{(h_t \\cdot \\theta)}} + (1 - y_t) * \\log{(1 - \\sigma{(h_t \\cdot \\theta)})} \\end{aligned} \\end{equation} $$\nIt's quite easy to prove that the logistic regression coefficient \\(\\hat{\\theta}\\) has a deterministic mapping to the optimal ranking Elo score where \\(E = C * \\hat{\\theta}\\). We give the rough intuition below. Suppose we only have a pair of model \\(i\\) and \\(j\\), out of all battles, we have \\(n\\) times model \\(i\\) wins and \\(m\\) times it loses. Thus the equation (2) becomes $$ \\begin{aligned} \\argmin_{\\theta} J \u0026= -n * \\log{\\sigma{(ln(k) * \\theta_i - ln(k) * \\theta_j)}} - m * \\log{(1 - \\sigma{(ln(k) * \\theta_i - ln(k) * \\theta_j)})} \\\\ \u0026= -\\frac{n}{n+m} * \\log{\\sigma{(ln(k) * \\theta_i - ln(k) * \\theta_j)}} - \\frac{m}{n+m} * \\log{(1 - \\sigma{(ln(k) * \\theta_i - ln(k) * \\theta_j)})} \\\\ \u0026= -P_{\\text{i wins j}} * \\log\\Bigg[\\frac{1}{1 + \\exp^{-(ln(k) * \\theta_i - ln(k) * \\theta_j)}}\\Bigg] - (1 - P_{\\text{ wins j}}) * \\log\\Bigg[1 - \\frac{1}{1 + \\exp^{-(ln(k) * \\theta_i - ln(k) * \\theta_j)}}\\Bigg] \\\\ \u0026= -P_{\\text{i wins j}} * \\log\\Bigg[\\frac{1}{1 + k^{\\theta_j - \\theta_i}}\\Bigg] - (1 - P_{\\text{ wins j}}) * \\log\\Bigg[1 - \\frac{1}{1 + k^{\\theta_j - \\theta_i}}\\Bigg] \\\\ \\end{aligned} $$\nlet \\(E = C * \\theta\\), we then have $$ \\begin{aligned} \\argmin_{\\theta} J \u0026= -P_{\\text{i wins j}} * \\log\\Bigg[\\frac{1}{1 + k^{(E_j - E_i)/C}}\\Bigg] - (1 - P_{\\text{ wins j}}) * \\log\\Bigg[1 - \\frac{1}{1 + k^{(E_j - E_i)/C}}\\Bigg] \\\\ \u0026= -P_{\\text{i wins j}} * \\log{\\hat{P}_{\\text{i wins j}}} - (1 - P_{\\text{i wins j}})* \\log{(1 - \\hat{P}_{\\text{i wins j}})} \\end{aligned} $$\nIt’s easy to see that the above equation is to calculate the cross-entropy between the Elo predicted win rate and the actual observed win rate. Thus, the best solution for equation (2) is the Elo score (or MLE coefficient) we are looking for.\nThis calculation is redone N times with bootstrapping to get confidence interval. N equals 100 in the case of the actual Arena implementation.\nSo what does all these calculation teaches us about the Elo score used in Chatbot Arena?\nFirst, because Bradley-Terry model is used, the model will treat the rating long time ago with the same weight as the it is recently. This might be okay for open source model where static weight is released. But for API based model, such evaluation can be problematic as previous rating is based on weak subversion than the latest most powerful release. That might be why companies like OpenAI and Google is using strange subversion names such as GPT-4o-2024-05-13 instead of just and umbrella version.\nSecondly, the score might hide the difference. The plot below is the win rate change as we change the Elo delta.\nAs can be seen here, even a 100 Elo delta renders the win rate from 50% to 65%. When you think about the model difference, I think it’s quite helpful to put it into the context of the actual win rate.\nLook Into Different Slice When you look at the overall slice, the number might suggest that a lot of models are similar. The following graph is a good example of this, this is a snapshot taken earlier this year (not as of time of writing). While it may seems that claude-3-opus is performing worse than llama-3-70b-instruct, when you limit to hard prompt, the performance clearly shows another picture. While the Elo rating is based on the overall uniform sampling of all the ratings, we as user almost always care more about the more challenging prompts. In the end, it doesn’t matter how well your model answers “how are you”, but instead how your model answers “help me translate this C++ program into Haskell”.\nI highly suggest anyone, when picking a model, take a look at the segment that you care about. I suggest definitely take a look at hard and coding, math as these segments correlates with the model’s reasoning capability strongly. For example, while ranked #1 in the overall segment, Gemini-1.5-Pro-Exp-0801’s coding capability significantly lags behind competitors, be careful when you choose it for your coding task.\nSummary I hope this articles provided you more detailed mechanisms for Chatbot Arena. Overall, it’s an open source platform where user across the world assign ratings to chatbot, and one leaderboard that ranks the model’s capability. This simple idea of using communities to help evaluate open-ended prompts turned out to be quite brilliant. But it is not without limitation. The most important limitation is bias, the users that actually coming to this website are majority software engineers and thus shifted this leaderboard heavily towards software engineering. Also the trustworthiness of the rater can be questionable sometimes, although the platform used many mechanisms to detect anormalous users, if you look at some of the safety ratings from the released Arena 33k, clearly the user prefers unsafe content even though the model should moderate sometimes.\nBut overall, I want you to appreciate the beauty of open source community and the power of simple ideas. Chatbot Arena has become the #1 leaderboard in the LLM industry. I hope this post helps you understand it a bit better!\n51.87% = 1/(1 + 10**((1286-1299)/400)) ↩︎\n",
  "wordCount" : "2198",
  "inLanguage": "en",
  "image": "https://realcwl.github.io/images/cover.jpeg","datePublished": "2024-08-07T00:00:00Z",
  "dateModified": "2024-08-07T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Weilun Chen"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://realcwl.github.io/posts/you_dont_know_lmsys/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Weilun's Homepage",
    "logo": {
      "@type": "ImageObject",
      "url": "https://realcwl.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://realcwl.github.io/" accesskey="h" title="Weilun&#39;s Homepage (Alt + H)">Weilun&#39;s Homepage</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://realcwl.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://realcwl.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://realcwl.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://realcwl.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://realcwl.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      You don&#39;t know Chatbot Arena
    </h1>
    <div class="post-description">
      Let&#39;s get a bit nerdy on the most trusted LLM leaderboard
    </div>
    <div class="post-meta"><span title='2024-08-07 00:00:00 +0000 UTC'>August 7, 2024</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;Weilun Chen&nbsp;|&nbsp;<a href="https://github.com/realcwl/realcwl.github.io/tree/main/content/posts/you_dont_know_lmsys.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#llm-evals-are-broken" aria-label="LLM evals are broken">LLM evals are broken</a></li>
                <li>
                    <a href="#chatbot-arena-in-a-nutshell" aria-label="Chatbot Arena in a nutshell">Chatbot Arena in a nutshell</a></li>
                <li>
                    <a href="#you-dont-know-chatbot-arena" aria-label="You don&rsquo;t know Chatbot Arena">You don&rsquo;t know Chatbot Arena</a><ul>
                        
                <li>
                    <a href="#understand-the-meaning-of-the-score" aria-label="Understand the meaning of the score">Understand the meaning of the score</a></li>
                <li>
                    <a href="#look-into-different-slice" aria-label="Look Into Different Slice">Look Into Different Slice</a></li></ul>
                </li>
                <li>
                    <a href="#summary" aria-label="Summary">Summary</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p><strong>Disclaimer</strong>: All of the following content are public information.</p>
<h2 id="llm-evals-are-broken">LLM evals are broken<a hidden class="anchor" aria-hidden="true" href="#llm-evals-are-broken">#</a></h2>
<p>It all starts with the question: How do you evaluate the quality of something that&rsquo;s as strong as GPT-4 (or Gemini).</p>
<p>Before the era of large language model, various of researchers spend many times constructing evaluation benchmark to evaluate the model&rsquo;s capability progress. I&rsquo;d argue that a good benchmark is what drive progress in the field of NLP, and claiming the lead on a benchmark usually comes with fame and fortune, driving researchers and companies to compete with each other to create a better model.</p>
<p>However, a lot of the benchmarks proposed in recent years are saturating in an unexpected speed. Take the following diagram as an example, it takes quite some time for a model to hill climb to human level performance in the first 15 years of this century, but just a few years (or months) for some of the newly proposed tasks such as <a href="https://gluebenchmark.com/leaderboard">GLUE</a>.</p>
<p><img loading="lazy" src="/lmsys/benchmark.jpeg" alt="benchmark saturating"  />
</p>
<p>Saturating is just one side of things. Another very annoying issue is <em>leakage</em>. It&rsquo;s actually quite easy to claim to the top of one leaderboard: You just train on the evaluation dataset. Such training can be intentional, we all know that there are enough dumb VCs who will just dumping money on you if you create a big headline on some leaderboard. But many times, the leakage is implicit and unintentional, with the massive training data, it&rsquo;s quite common that you scraped some 3rd party website that happen to include a large chunk of MMLU data and by pretraining on it your model just remembers the answer. There are ways to detect such leakage, for example one way of detecting such leakage is by measuring the model&rsquo;s perplexity on the eval set, and if model is more confident than some threshold we deem the eval set is leaked.</p>
<p><img loading="lazy" src="/lmsys/detect.jpeg" alt="leakage detection"  />
</p>
<p>Another solution is to create benchmark that&rsquo;s either hidden or dynamic. For example, ScaleAI has <a href="https://scale.com/leaderboard">SEAL leaderboard</a> on which they constantly conduct evaluation to measure the model&rsquo;s capability. But as ScaleAI is also creating data for all these large language model providers, it&rsquo;s not so convincing that they will actually adversarially exposing their clients model weakness. Another type of evaluation is to make it dynamic, with periodic refresh of the problem. <a href="https://livecodebench.github.io">LiveCodeBench</a> is such a project where new problems are published constantly to ensure the freshness of data.</p>
<p>However, even if we spend a lot of effort curating a very good dataset, we still face a quite serious problem: all of the aforementioned evaluations are quite artificial. A normal user doesn&rsquo;t just go to LLM to ask leetcode questions, or asking a high school math selection problems. A user&rsquo;s goal for the LLM is quite simple: <strong>ask real life questions and want a high quality answer</strong>. How can we make the evaluation focus on this front, while it&rsquo;s quite challenging to effectively measure the open-ended answers?</p>
<h2 id="chatbot-arena-in-a-nutshell">Chatbot Arena in a nutshell<a hidden class="anchor" aria-hidden="true" href="#chatbot-arena-in-a-nutshell">#</a></h2>
<p>Right after ChatGPT released, a few students in Berkeley Sky Computing Lab started to replicate ChatGPT&rsquo;s success by creating an open source large language model called <a href="https://lmsys.org/blog/2023-03-30-vicuna/">Vicuna</a>. During the development of Vicuna, they started to realize it&rsquo;s quite hard to comprehensively evaluate the large language model. To solve this issue, they began to develop an open platform that will later become <a href="https://chat.lmsys.org/">Chatbot Arena</a>, with a simple hypothesis: &ldquo;human are good at judging quality of LLM if presented side by side&rdquo;.</p>
<p>When the user comes to the website, they can issue queries just like using any LLM provider. The query will be sent to 2 &ldquo;random&rdquo; LLMs to get responses. And then presented to the user for rating out of 4 options: <strong>1)</strong> left is better. <strong>2)</strong> right is better.  <strong>3)</strong> tie.  <strong>4)</strong> tie (both bad).</p>
<p><img loading="lazy" src="/lmsys/arena_demo.png" alt="arena demo"  />
</p>
<p>
After this rating steps. Arena will gather all the votes and rank the LLMs under comparison. How? Fortunately, human is a competitive animal, and we have already developed many ways to rank stuff. One of the most commonly used ranking systems is <a href="https://en.wikipedia.org/wiki/Elo_rating_system">ELO ranking</a>. Assuming 2 players in a zero sum game with ELO score <span>\(R_A\)</span> and <span>\(R_B\)</span>. The probability of player <span>\(A\)</span> wins over player <span>\(B\)</span> is expressed as: 
</p>
<p>$$
\begin{equation}
\begin{aligned}
P(A &gt; B) &amp;= \frac{1}{1 + 10^{(R_B-R_A)/400}}
\end{aligned}
\end{equation}
$$</p>
<p>By looking at all the comparisons submitted by users, we can derive one score for each model to best approximate the above probability. The following leaderboard is derived at 2024-08-11. As shown in Equation (1), the probability of <code>Gemini-1.5-Pro-Exp-0801</code> being greater than <code>GPT-4o-2024-05-13</code> is 51.87%<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p><img loading="lazy" src="/lmsys/leaderboard.jpeg" alt="leaderboard"  />
</p>
<p>As of the time of writing, this platform has collected comparisons across 127 different models, and a total votes of 1,610,507, and it has become the #1 trusted leaderboard for comparing and understand the strength of all the LLMs. There has been changes to this leaderboard such as introducing <a href="https://lmsys.org/blog/2024-05-17-category-hard/">hard categories</a>, <a href="https://lmsys.org/blog/2024-06-27-multimodal/">multi-modal</a> but the core idea is similar. That&rsquo;s it about Chatbot Arena! A single open platform that open internet users can come and rate LLM on their questions, and a unified leaderboard to rank LLMs by their capabilities!</p>
<h2 id="you-dont-know-chatbot-arena">You don&rsquo;t know Chatbot Arena<a hidden class="anchor" aria-hidden="true" href="#you-dont-know-chatbot-arena">#</a></h2>
<p>What&rsquo;s more to understand this leaderboard besides it&rsquo;s a comparison platform and some magics to calculate the scores? I&rsquo;d argue that there are many more details in the implementation of this leaderboard and can provide you insights on why some phenomenon exist. Most of the content below is my takeaway after reading their <a href="https://arxiv.org/abs/2403.04132">2024 ICML paper</a>.</p>
<h3 id="understand-the-meaning-of-the-score">Understand the meaning of the score<a hidden class="anchor" aria-hidden="true" href="#understand-the-meaning-of-the-score">#</a></h3>
<p>First, what does Arena score actually mean? Let&rsquo;s take the leaderboard as an example. Gemini has an Elo score of 1299, while Claude 3.5 Sonnet has an Elo score of 1271. The actual meaning of these 2 score can be translated with equation (1) we derived above. That is,</p>
<p>$$
\begin{aligned}
P(\textbf{user prefer Gemini than Claude}) &amp;= \frac{1}{1 + 10^{(R_{claude}-R_{gemini})/400}} \\
&amp;= \frac{1}{1 + 10^{(1271-1299)/400}} \\
&amp;= 0.54
\end{aligned}
$$</p>
<p>In another words, this is saying that <strong>on the queries coming from Arena users</strong>, Gemini wins Claude 3.5 54% of times.</p>
<p>
To go into more details, we want to ask, how is this score calculated? A common misconception is that this Elo score is calculated based on online Elo ranking, same as how tennis/chess player's Elo is calculated in the equation below, where all the battles are sorted chronologically and one by one feed into the system for calculation. In the equation below, \(S_A\) is the actual result of the battle, while \(E_A\) is the predicted result from equation (1).
</p>
<p>$$
R^{&rsquo;}_{A} = R_A + K \cdot (S_A - E_A)
$$</p>
<p>Chatbot Arena was initially using online Elo system for the ranking with a K-factor of 4, but online Elo system will prefer recent results more than the historical results. This will cause instability in the calculation, for example if you calculate the results twice with reverse ordering, the ranking will become quite different:</p>
<p><img loading="lazy" src="/lmsys/reverse_ranking.jpeg" alt="reverse_ranking"  />
</p>
<p>Instead, Chatbot Arena today is using Maximum Likelihood Estimation with <a href="https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model">Bradley-Terry model</a> to calculate the actual ranking. It reformulates the calculation of the final Elo score into the following MLE problem with logistic regression.</p>
<p>
Suppose we have \(M\) models under comparison, and there is no tie. Each model has an Elo estimation \(E\), the base of Elo is \(k\) (\(k=10\) as in the case of equation 1), and scale factor of \(C\) (\(C=400\) in equation (1)) and we have \(T\) observations of the outcome of battle, each denoted as a \(M\) length of vector \(h\) where the ith element has value
</p>
<p>$$
h_{t}^{i} = \begin{cases}
ln(k) &amp; \text{when ith model is the model a in battle t}  \\
-ln(k) &amp; \text{when ith model is the model b in battle t} \\
0 &amp; \text{when ith model doesn&rsquo;t participate the battle t}
\end{cases}
$$</p>
<p>
Finally, we let \(y_{1...T}\) be our target ground truth, where
</p>
<p>$$
y_{t} = \begin{cases}
1 &amp; \text{when model a is the winner in battle t}  \\
0 &amp; \text{when model b is the winner in battle t} \\
\end{cases}
$$</p>
<p>
We fit the following logistic regression model with no bias term and \(M\)-length model parameter \(\theta\)
</p>
<p>$$
\begin{equation}
\begin{aligned}
\hat{\theta} &amp;= \argmin_\theta{J(\theta)} \\
&amp;= \argmin_{\theta}-\sum_{t=1}^{T} y_t * \log{\sigma{(h_t \cdot \theta)}} + (1 - y_t) * \log{(1 - \sigma{(h_t \cdot \theta)})}
\end{aligned}
\end{equation}
$$</p>
<p>
It's quite easy to prove that the logistic regression coefficient \(\hat{\theta}\) has a deterministic mapping to the optimal ranking Elo score where \(E = C * \hat{\theta}\). We give the rough intuition below.
</p>
<p>
Suppose we only have a pair of model \(i\) and \(j\), out of all battles, we have \(n\) times model \(i\) wins and \(m\) times it loses. Thus the equation (2) becomes
</p>
<p>$$
\begin{aligned}
\argmin_{\theta} J &amp;= -n * \log{\sigma{(ln(k) * \theta_i - ln(k) * \theta_j)}} - m * \log{(1 - \sigma{(ln(k) * \theta_i - ln(k) * \theta_j)})} \\
&amp;= -\frac{n}{n+m} * \log{\sigma{(ln(k) * \theta_i - ln(k) * \theta_j)}} - \frac{m}{n+m} * \log{(1 - \sigma{(ln(k) * \theta_i - ln(k) * \theta_j)})} \\
&amp;= -P_{\text{i wins j}} * \log\Bigg[\frac{1}{1 + \exp^{-(ln(k) * \theta_i - ln(k) * \theta_j)}}\Bigg] - (1 - P_{\text{ wins j}}) * \log\Bigg[1 - \frac{1}{1 + \exp^{-(ln(k) * \theta_i - ln(k) * \theta_j)}}\Bigg] \\
&amp;= -P_{\text{i wins j}} * \log\Bigg[\frac{1}{1 + k^{\theta_j - \theta_i}}\Bigg] - (1 - P_{\text{ wins j}}) * \log\Bigg[1 - \frac{1}{1 + k^{\theta_j - \theta_i}}\Bigg] \\
\end{aligned}
$$</p>
<p>
let \(E = C * \theta\), we then have
</p>
<p>$$
\begin{aligned}
\argmin_{\theta} J &amp;= -P_{\text{i wins j}} * \log\Bigg[\frac{1}{1 + k^{(E_j - E_i)/C}}\Bigg] - (1 - P_{\text{ wins j}}) * \log\Bigg[1 - \frac{1}{1 + k^{(E_j - E_i)/C}}\Bigg] \\
&amp;= -P_{\text{i wins j}} * \log{\hat{P}_{\text{i wins j}}} - (1 - P_{\text{i wins j}})*  \log{(1 - \hat{P}_{\text{i wins j}})}
\end{aligned}
$$</p>
<p>It&rsquo;s easy to see that the above equation is to calculate the cross-entropy between the Elo predicted win rate and the actual observed win rate. Thus, the best solution for equation (2) is the Elo score (or MLE coefficient) we are looking for.</p>
<p>This calculation is redone N times with <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">bootstrapping</a> to get confidence interval. N equals 100 in the case of the actual Arena implementation.</p>
<p>So what does all these calculation teaches us about the Elo score used in Chatbot Arena?</p>
<p>First, because Bradley-Terry model is used, the model will treat the rating long time ago with the same weight as the it is recently. This might be okay for open source model where static weight is released. But for API based model, such evaluation can be problematic as previous rating is based on weak subversion than the latest most powerful release. That might be why companies like OpenAI and Google is using strange subversion names such as <code>GPT-4o-2024-05-13</code> instead of just and umbrella version.</p>
<p>Secondly, the score might hide the difference. The plot below is the win rate change as we change the Elo delta.</p>
<p><img loading="lazy" src="/lmsys/winrate_by_delta.png" alt="elo_delta"  />
</p>
<p>As can be seen here, even a 100 Elo delta renders the win rate from 50% to 65%. When you think about the model difference, I think it&rsquo;s quite helpful to put it into the context of the actual win rate.</p>
<h3 id="look-into-different-slice">Look Into Different Slice<a hidden class="anchor" aria-hidden="true" href="#look-into-different-slice">#</a></h3>
<p>When you look at the overall slice, the number might suggest that a lot of models are similar. The following graph is a good example of this, this is a snapshot taken earlier this year (not as of time of writing). While it may seems that <code>claude-3-opus</code> is performing worse than <code>llama-3-70b-instruct</code>, when you limit to <a href="https://lmsys.org/blog/2024-04-19-arena-hard/">hard prompt</a>, the performance clearly shows another picture. While the Elo rating is based on the overall uniform sampling of all the ratings, we as user almost always care more about the more challenging prompts. In the end, it doesn&rsquo;t matter how well your model answers &ldquo;how are you&rdquo;, but instead how your model answers &ldquo;help me translate this C++ program into Haskell&rdquo;.</p>
<p><img loading="lazy" src="/lmsys/hard_elo.png" alt="hard_elo"  />
</p>
<p>I highly suggest anyone, when picking a model, take a look at the segment that you care about. I suggest definitely take a look at <code>hard</code> and <code>coding</code>, <code>math</code> as these segments correlates with the model&rsquo;s reasoning capability strongly. For example, while ranked #1 in the overall segment, <code>Gemini-1.5-Pro-Exp-0801</code>&rsquo;s coding capability significantly lags behind competitors, be careful when you choose it for your coding task.</p>
<p><img loading="lazy" src="/lmsys/gemini_code.jpeg" alt="gemini_coding"  />
</p>
<h2 id="summary">Summary<a hidden class="anchor" aria-hidden="true" href="#summary">#</a></h2>
<p>I hope this articles provided you more detailed mechanisms for Chatbot Arena. Overall, it&rsquo;s an open source platform where user across the world assign ratings to chatbot, and one leaderboard that ranks the model&rsquo;s capability. This simple idea of using communities to help evaluate open-ended prompts turned out to be quite brilliant. But it is not without limitation. The most important limitation is bias, the users that actually coming to this website are majority software engineers and thus shifted this leaderboard heavily towards software engineering. Also the trustworthiness of the rater can be questionable sometimes, although the platform used many mechanisms to detect <a href="https://arxiv.org/html/2403.04132v1#:~:text=Detecting%20Anomalous%20Users">anormalous users</a>, if you look at some of the safety ratings from the released <a href="https://huggingface.co/datasets/lmsys/chatbot_arena_conversations">Arena 33k</a>, clearly the user prefers unsafe content even though the model should moderate sometimes.</p>
<p>But overall, I want you to appreciate the beauty of open source community and the power of simple ideas. Chatbot Arena has become the #1 leaderboard in the LLM industry. I hope this post helps you understand it a bit better!</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>51.87% = 1/(1 + 10**((1286-1299)/400))&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://realcwl.github.io/tags/llm/">LLM</a></li>
      <li><a href="https://realcwl.github.io/tags/evaluation/">Evaluation</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://realcwl.github.io/posts/rlhf_to_ipo/">
    <span class="title">Next »</span>
    <br>
    <span>From RLHF to Direct Preference Learning</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share You don&#39;t know Chatbot Arena on x"
            href="https://x.com/intent/tweet/?text=You%20don%27t%20know%20Chatbot%20Arena&amp;url=https%3a%2f%2frealcwl.github.io%2fposts%2fyou_dont_know_lmsys%2f&amp;hashtags=LLM%2cEvaluation">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share You don&#39;t know Chatbot Arena on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frealcwl.github.io%2fposts%2fyou_dont_know_lmsys%2f&amp;title=You%20don%27t%20know%20Chatbot%20Arena&amp;summary=You%20don%27t%20know%20Chatbot%20Arena&amp;source=https%3a%2f%2frealcwl.github.io%2fposts%2fyou_dont_know_lmsys%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share You don&#39;t know Chatbot Arena on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2frealcwl.github.io%2fposts%2fyou_dont_know_lmsys%2f&title=You%20don%27t%20know%20Chatbot%20Arena">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share You don&#39;t know Chatbot Arena on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frealcwl.github.io%2fposts%2fyou_dont_know_lmsys%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share You don&#39;t know Chatbot Arena on whatsapp"
            href="https://api.whatsapp.com/send?text=You%20don%27t%20know%20Chatbot%20Arena%20-%20https%3a%2f%2frealcwl.github.io%2fposts%2fyou_dont_know_lmsys%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share You don&#39;t know Chatbot Arena on telegram"
            href="https://telegram.me/share/url?text=You%20don%27t%20know%20Chatbot%20Arena&amp;url=https%3a%2f%2frealcwl.github.io%2fposts%2fyou_dont_know_lmsys%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share You don&#39;t know Chatbot Arena on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=You%20don%27t%20know%20Chatbot%20Arena&u=https%3a%2f%2frealcwl.github.io%2fposts%2fyou_dont_know_lmsys%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer><script src="https://giscus.app/client.js"
        data-repo="realcwl/realcwl.github.io"
        data-repo-id="R_kgDOMHPdlg"
        data-category="General"
        data-category-id="DIC_kwDOMHPdls4CgorP"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://realcwl.github.io/">Weilun&#39;s Homepage</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
