<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>From RLHF to DPO | Weilun&#39;s Homepage</title>
<meta name="keywords" content="LLM, Reinforcement Learning">
<meta name="description" content="How does RLHF and DPO works under the hood, and where the limitation is">
<meta name="author" content="Weilun Chen">
<link rel="canonical" href="http://localhost:1313/posts/rlhf_to_ipo/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5ff2630c4d1b3e25bc21f0ecd96681dbcf58219e741fa627857820b5485cb770.css" integrity="sha256-X/JjDE0bPiW8IfDs2WaB289YIZ50H6YnhXggtUhct3A=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/rlhf_to_ipo/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

  

<meta property="og:title" content="From RLHF to DPO" />
<meta property="og:description" content="How does RLHF and DPO works under the hood, and where the limitation is" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/posts/rlhf_to_ipo/" />
<meta property="og:image" content="http://localhost:1313/images/papermod-cover.png" />
<meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-07-04T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-07-04T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="http://localhost:1313/images/papermod-cover.png" />
<meta name="twitter:title" content="From RLHF to DPO"/>
<meta name="twitter:description" content="How does RLHF and DPO works under the hood, and where the limitation is"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "From RLHF to DPO",
      "item": "http://localhost:1313/posts/rlhf_to_ipo/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "From RLHF to DPO",
  "name": "From RLHF to DPO",
  "description": "How does RLHF and DPO works under the hood, and where the limitation is",
  "keywords": [
    "LLM", "Reinforcement Learning"
  ],
  "articleBody": "It’s well known that the state-of-the-art LLM models are trained with massive human quality feedback. This feedback is either coming from a massive rater pool, or can come from the end users implicitly (sometimes explicitly as well.. remember when ChatGPT presents you 2 responses to choose from?).\nHowever, I found that there are many subtleties for one to truly understand what’s going on under the hood. This is a blog to capture my understanding of the RLHF algorithm, and how it evolves into rewardless model such as DPO and IPO.\nA quick recap of RLHF Suppose our reward is point-wise, meaning we get a single float number to denote the quality of the response based on the context, we try to optimize for the following objective in RLHF\n$$ \\begin{equation} \\begin{aligned} J(\\pi) \u0026= \\mathbb{E}_{\\substack{x \\sim D \\\\ y \\sim \\pi(\\cdot|x)}} \\left[ r(x, y) \\right] - \\beta\\mathbb{D}_\\text{KL}(\\pi ||\\pi_\\text{ref}) \\end{aligned} \\end{equation} $$\nThe reward function \\( r(x, y) \\) usually isn't directly differentiable in relative to \\(\\pi\\) itself (for example, in auto-regressive model where we sample each token with argmax). Thus the most common approach to deal with this case is to use actor critic method to find the best policy. Optimizing equation (1) usually consists of the following general steps, assuming we are using PPO\n1. Training the reward model\nSuppose we have access of many \\( (\\text{prompt}, \\text{likert score}) \\) pairs, we can train the reward model \\( r(x, y) \\) with MSE loss and with a neural network. One good thing is that such neural network can be directly initialized with the underlying policy transformer network, and replace the output layers to be a shallow FFN. 2. Sample the policy, and estimate value\nDuring this step, suppose we have access to many prompts that matches the distribution we want to optimize, say \\( x \\sim D \\), we can thus run the policy \\(T\\) times and send the responses for reward scoring. This will results a distribution \\( \\text{M} \\) with \\(T\\) episodes where each episode \\( \\text{M}^t \\) consists of: $$ \\text{M}^t = (\\text{prompt x}^t, \\text{response y}^t, \\text{score r}^t, \\text{value v}^t, \\text{probability p}^t) $$\nAdditionally, we also generate 2 more things:\nvalue: This is generated by a value network, which is needed to stabilize the RL training to make it offset invariant. See PPO paper for details. One good thing about it is that such value network can be initialized by the policy SFT’ed network too, similar to our reward model. probability: This is the probability of the response sequence (of length n), equals to $$ \\prod_{i=1}^{n} p(y_i \\mid y_1, y_2, \\ldots, y_{i-1}, x) $$ this will later be used to calculate the KL divergence. 3. Train with PPO\nFollowing the common PPO training, we have the loss function across all the \\(T\\) episodes as $$ \\begin{aligned} L^{\\text{PPO}}(\\pi) = \\mathbb{E}_{t \\sim M} \\Bigg[ \u0026 \\min \\left( \\rho_t(\\pi) \\hat{A}_t, \\text{clip} \\left( \\rho_t(\\pi), 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{A}_t \\right) \\Bigg] \\end{aligned} $$\nwhere, $$ \\begin{aligned} \\rho_t(\\pi) \u0026= \\frac{\\pi(y^t | x^t)}{\\pi_{\\text{ref}}(y^t | x^t)} \\\\ \\hat{A}_t \u0026= J_t - v^t \\\\ J_t \u0026= r^t - \\beta\\mathbb{D}_{\\text{KL}}(\\pi || \\pi_{\\text{ref}}) \\end{aligned} $$\nI think it’s worth pointing out the calculation of the KL divergence (at least it took me a long time to figure out). The definition of KL divergence is\n$$ D_{\\text{KL}}(\\pi \\parallel \\pi_{\\text{ref}}) = \\sum_{y} \\pi(y|x) \\log \\left( \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} \\right) $$\ncomprehensively evaluate for all y is computationally intractable (like, you need to exhaust all possible human sentences). But recall that the meaning of KL divergence is measure how different 2 distribution is, and that can also be translated to how different 2 distribution assign probability to the same sequence sample from one of them1.\nThus a noisy estimation of the KL divergence can be expressed as\n$$ D_{\\text{KL}}(\\pi \\parallel \\pi_{\\text{ref}}) = \\mathbb{E}_{x \\sim M}[\\text{KL}(\\pi(\\cdot|x) || \\pi_{\\text{ref}}(\\cdot|x))] $$\nwhere\n$$ \\mathbb{E}_{x \\sim M}[\\text{KL}(\\pi(\\cdot|x) || \\pi_{\\text{ref}}(\\cdot|x))] = \\mathbb{E}_{y \\sim \\pi(\\cdot|x)}\\Bigg[\\log(\\frac{\\pi(y|x)}{\\pi_{\\text{ref}(y|x)}}) \\Bigg] $$\nYou can also check this implementation to gain a deeper understanding of it.\n4. Update value estimator\nFinally, once the model has been updated, the value function is fitting to the actual reward. You repeat 2-4 until the model is aligned with the preference.\nDeal with pairwise preference The previous section is assuming a point wise preference labeling, that is, each reward labeling is of pair $$ (\\text{prompt}, \\text{likert score}) $$ However, human is not very good at rating absolutely, but very good as comparing things side by side. For example, it’s hard to say how hot the weather is exactly in terms of Celsius, but it’s quite easy to tell the the weather in the evening is more chill compared to the afternoon. The same intuition applies for LLM training, while it’s hard to collect absolute likert score, it’s relatively easy to collect a triplet of $$ (\\text{prompt}, \\text{response}_w, \\text{response}_l) $$\nWhere the 2 responses are 2 samples and we collect the preference score from a rater pool about which one is better (\\(\\text{response}_w\\)) and which one is worse (\\(\\text{response}_l\\)) in the context of the prompt. Rather than training the reward model to fit the absolute score, the reward model instead trained to fit a Bradley-Terry model 2.\nLet the prompt be \\(x\\) and \\( \\text{response}_w, \\text{response}_l \\) be \\(y_w, y_l\\) respectively. The objective instead becomes $$ L(r) = -\\mathbb{E}_{(x, y_w, y_l)}[\\log(p(y_w\u003ey_l|x))] $$ where $$ p(y_w\u003ey_l|x) = \\sigma(r(x,y_w) - r(x, y_l)) $$\nSee this implementation to get a deeper understanding of the pairwise reward model training. Once the reward model is trained, we use PPO with the same objective as in equation (1) to perform RLHF.\nDPO: Avoid the reinforcement learning at all Let’s recap, in RLHF (with PPO), we need to bring up 3 models to do different things. We need our policy model to create episodes and update, a reward model to score the generated response and a value model to estimate the value in order to stabilize the training. Although theoretically a lot of parameters can be shared across them 3, it is still relatively resource intensive compared to instruction fine tuning. In addition, it requires an additional step to train the reward model, which leaves more room for error.\nThus, we are interested in a reward free method which can be more resource/time efficient method that can avoid the above issue. And guess what, DPO does exact that with a quite beautiful mathematic trick.\nRecall that our objective, even though non differentiable, is $$ \\begin{aligned} J(\\pi) \u0026= \\mathbb{E}_{\\substack{x \\sim D \\\\ y \\sim \\pi(\\cdot|x)}} \\left[ r(x, y) \\right] - \\beta\\mathbb{D}_\\text{KL}(\\pi ||\\pi_\\text{ref}) \\end{aligned} $$ Let’s see if we can have an analytic solution to the optimal policy $$ \\begin{aligned} \\max_\\pi J \u0026= \\max_\\pi\\mathbb{E}_{\\substack{x \\sim D \\\\ y \\sim \\pi(\\cdot|x)}} \\left[ r(x, y) \\right] - \\beta\\mathbb{D}_\\text{KL}(\\pi ||\\pi_\\text{ref}) \\\\ \u0026= \\max_\\pi\\mathbb{E}_{\\substack{x \\sim D \\\\ y \\sim \\pi(\\cdot|x)}} \\left[ r(x, y) \\right] - \\beta\\mathbb{E}_{\\substack{x \\sim D \\\\ y \\sim \\pi(\\cdot|x)}}\\Bigg[\\log(\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)})\\Bigg] \\\\ \u0026= \\max_\\pi\\mathbb{E}_{\\substack{x \\sim D \\\\ y \\sim \\pi(\\cdot|x)}} \\Bigg[ r(x, y) - \\beta\\log(\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)})\\Bigg] \\\\ \u0026= \\min_\\pi\\mathbb{E}_{\\substack{x \\sim D \\\\ y \\sim \\pi(\\cdot|x)}} \\Bigg[ \\beta\\log(\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)}) - r(x, y) \\Bigg] \\\\ \u0026= \\min_\\pi\\mathbb{E}_{\\substack{x \\sim D \\\\ y \\sim \\pi(\\cdot|x)}} \\Bigg[ \\beta\\log(\\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\cdot\\exp(\\frac{1}{\\beta}r(x, y))}) \\Bigg] \\\\ \u0026= \\beta\\min_\\pi\\mathbb{E}_{\\substack{x \\sim D \\\\ y \\sim \\pi(\\cdot|x)}} \\Bigg[ \\log{\\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\cdot\\exp(\\frac{1}{\\beta}r(x, y))}} - \\log{Z(x)}\\Bigg] \\\\ \\end{aligned} $$\nlet \\(Z(x) = \\sum_{\\text{all of } y \\sim \\pi(\\cdot|x)}{\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x, y))} \\), we can see that the denominator in the above equation is a valid distribution (probability adds up to 1). Thus, the objective function becomes $$ \\begin{equation} \\begin{aligned} \\max_\\pi J \u0026= \\beta\\min_\\pi\\mathbb{E}_{x \\sim D } [\\mathbb{D}_\\text{KL}(\\pi(y|x) ||\\pi_*(y|x)) - \\log{Z(x)}] \\end{aligned} \\end{equation} $$ where $$ \\begin{equation} \\pi_*(y|x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\cdot\\exp(\\frac{1}{\\beta}r(x, y)) \\end{equation} $$ because \\(Z(x)\\) is exactly irrelevant to \\(\\pi\\), we know that solution to equation (2) is \\(\\pi = \\pi_*\\). Thus equation (3) is our optimal policy model that perfectly aligns with the reward model. Up until know, our conclusion in (3) doesn't help so much. That's because we have a nasty term \\(Z(x)\\) which requires up to calculate the probability of all possible y given x which is impractical in practice. But here comes the fun part: With some mathematic tricks, we can cancel out the incomputable Z and get a computable loss function!:\nTo see how, let's assume our reward model \\(r^*\\) fits the human preference with a Bradley-Terry model. That is, we have $$ \\begin{equation} p(y_w\u003ey_l|x) = \\sigma(r(x, y_w) - r(x, y_l)) \\end{equation} $$ Because we have \\(\\pi_*(y|x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\cdot\\exp(\\frac{1}{\\beta}r(x, y))\\), we can reparametrize the optimal reward as $$ \\begin{equation} r(x, y) = \\beta\\log{\\frac{\\pi^*(y|x)}{\\pi_{\\text{ref}}(y|x)}} + \\beta\\log{Z(x)} \\end{equation} $$ If we substitute (5) into (4), surprisingly we canceled out the term Z! And the Bradley-Terry model can be expressed as $$ \\begin{equation} p(y_w\u003ey_l|x) = \\sigma\\Bigg(\\beta\\log{\\frac{\\pi^*(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}} - \\beta\\log{\\frac{\\pi^{\\ast}(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}}\\Bigg) \\end{equation} $$\nI just want to stop here and appreciate the beauty of the above derivation. Let’s think about what this final equation (6) actually implies:\nFinding the perfect reward model that fits the pair wise preference in (4) is mathematically equivalent to finding a perfect policy model that fits the same pair wise preference. Any reward model has a corresponding policy model that captures the same learned preference (equation (3)) Thus, our final objective becomes $$ \\begin{equation} L_{\\text{DPO}}(\\pi;\\pi_{\\text{ref}})=-\\mathbb{E}_{(x,y_w,y_l) \\sim D}\\Bigg[ \\log{\\sigma\\Bigg(\\beta\\log{\\frac{\\pi^*(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}} - \\beta\\log{\\frac{\\pi^{\\ast}(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}}\\Bigg)} \\Bigg] \\end{equation} $$\nNote that the above objective (7) is differentiable, and thus we don’t need reinforcement learning to learn the objective, but instead can just train the policy model with back propagation! This thus greatly simplifies our training process.\nWe suggest you to also look at the original DPO paper where it analyze the property of its gradient to build more intuition. But to sum up, the policy model together with the original reference model (SFT model) encodes implicitly the preference in the reward model training dataset. Such preference doesn’t require training a RM as a proxy but instead can directly train the policy model to learn. Due to such simplicity, in fact most of the open source large language model use some version of DPO to train the model.\nLimitation of DPO But is DPO the perfect method to train LLM? Why we never heard that OpenAI is using DPO to perform GPT post-training?\nMany works456 analyze the problem, and shows that DPO has the following theoretical limitations:\nDPO will assign high value to out-of-distribution samples 4 Suppose we have only 3 actions listed in the table below\nAction y1 y2 y3 πref 0.5 0.5 0 Dpref {(yw = y1, yl = y2)} πDPO 0.1 0.0 0.9 πPPO 1 0 0 We can see that DPO can learn the solution of 0.1, 0, 0.9. This is because the reward function\n$$ \\begin{aligned} L_{\\text{DPO}}(\\pi;\\pi_{\\text{ref}})\u0026 =-\\mathbb{E}_{(x,y_w,y_l) \\sim D}\\Bigg[ \\log{\\sigma\\Bigg(\\beta\\log{\\frac{\\pi^*(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)}} - \\beta\\log{\\frac{\\pi^{\\ast}(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}}\\Bigg)} \\Bigg] \\\\ \u0026= -\\log{\\sigma\\Bigg(\\beta(\\log(\\frac{a}{0.5})-\\log{\\frac{b}{0.5}})\\Bigg)} \\\\ \u0026= \\log\\Bigg(1+(\\frac{a}{b})^\\beta\\Bigg) \\end{aligned} $$\nThus as long as \\(a=0\\), it is an optimal policy from the view of DPO. However, such policy won't be an optimal one for PPO training, as the KL divergence will ensure the model to stay close to the reference model. This is also discovered in the Nvidia Nemotron5 training, where in the tech report, they reported that\nWhile the policy learns to differentiate chosen and rejected responses, we observe both chosen and rejected responses’ likelihoods drop consistently with their gap increasing, even if chosen responses are high-quality.\nIn addition, RLHF with PPO can leverage prompt only date, and such dataset act as a good regularizer of the model training to avoid it bias toward OOD samples.\nDPO will overfit when the preference is too deterministic In DeepMind's analysis, under the assumption that \\(p^\\ast\\) fits a Bradley-Terry model, we can prove that for the learning objective below $$ \\max_\\pi\\mathbb{E}_{\\substack{x \\sim D \\\\ y \\sim \\pi(\\cdot|x) \\\\ y’ \\sim \\mu(\\cdot|x)}}[\\Psi(p^\\ast(y\u003ey’|x))] - \\tau\\mathbb{D}_{\\text{KL}}(\\pi || \\pi_{\\text{ref}}) $$\nit is exactly the same as the RLHF objective in (1), and thus equivalent to the DPO learning objective in (7), when $$ \\Psi(q) = \\log(\\frac{q}{1-q}) $$\nIt's actually quite easy to prove this proposition and we leave that to the reader. But looking at the shape of this function, we can see that this function has a very large gradient near the point of 0 and 1. When training with this objective, a small increase in the preference near 0 or 1 will just be as incentivized as a large increase near 0.5. Thus, when the ground truth label is very deterministic (such as machine based feedback), thus no matter how large the regularization term \\(\\tau\\) is, the model will eventually deviate away from the reference and thus overfit drastically. This is also demonstrated in the experiment of IPO, where no matter how large the regularizer is, the policy will eventually deviate away from the original one. However, this won’t happen in RLHF not only because a RM is trained to proxy the raw preference and serves as a regularizer, it utilize PPO which only moves the policy in a trust region. This might be why DeepSeek V2 Coder is using an RM to regularize the execution feedback instead of sending it directly for learning.\nSummary Okay, you made it to the end, congrats :) Before you go, let’s just recap what we have learned:\nRLHF with PPO is the standard way of aligning the post SFT model towards annotated human preference. Pairwise preference is fitted with a Bradley-Terry model. DPO utilize a reparametrization trick, and prove that theoretically the policy model captures the preference and can be directly learned via back propagation. While DPO is simpler and more computational efficient, it suffers from regularization problem for both OOD and deterministic feedback (such as 0/1), thus while it generally performs well in more subjective tasks where a multi-scale likert score is used, it should be used with caution when the feedback is too deterministic or preference dataset is too small. I hope you like this post and please don’t hesitate to give your suggestion (scroll all the way up and click “Suggest Changes”). Thanks!\nSee this video for a very good explanation with coin toss. ↩︎\nhttps://web.stanford.edu/class/archive/stats/stats200/stats200.1172/Lecture24.pdf ↩︎\nGoogle Sparrow: https://arxiv.org/abs/2209.14375 ↩︎\nDPO will increase the likelihood of sample that’s OOD of preference pairs https://arxiv.org/abs/2404.10719 ↩︎ ↩︎\nNvidia NemoTron Tech Report: https://arxiv.org/html/2406.11704v1 ↩︎ ↩︎\nGoogle IPO: https://arxiv.org/abs/2310.12036 ↩︎\n",
  "wordCount" : "2339",
  "inLanguage": "en",
  "image": "http://localhost:1313/images/papermod-cover.png","datePublished": "2024-07-04T00:00:00Z",
  "dateModified": "2024-07-04T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Weilun Chen"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/rlhf_to_ipo/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Weilun's Homepage",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Weilun&#39;s Homepage (Alt + H)">Weilun&#39;s Homepage</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      From RLHF to DPO
    </h1>
    <div class="post-description">
      How does RLHF and DPO works under the hood, and where the limitation is
    </div>
    <div class="post-meta"><span title='2024-07-04 00:00:00 +0000 UTC'>July 4, 2024</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;Weilun Chen&nbsp;|&nbsp;<a href="https://github.com/realcwl/realcwl.github.io/tree/main/content/posts/rlhf_to_ipo.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#a-quick-recap-of-rlhf" aria-label="A quick recap of RLHF">A quick recap of RLHF</a></li>
                <li>
                    <a href="#deal-with-pairwise-preference" aria-label="Deal with pairwise preference">Deal with pairwise preference</a></li>
                <li>
                    <a href="#dpo-avoid-the-reinforcement-learning-at-all" aria-label="DPO: Avoid the reinforcement learning at all">DPO: Avoid the reinforcement learning at all</a></li>
                <li>
                    <a href="#limitation-of-dpo" aria-label="Limitation of DPO">Limitation of DPO</a><ul>
                        
                <li>
                    <a href="#dpo-will-assign-high-value-to-out-of-distribution-samples-4" aria-label="DPO will assign high value to out-of-distribution samples 4">DPO will assign high value to out-of-distribution samples 4</a></li>
                <li>
                    <a href="#dpo-will-overfit-when-the-preference-is-too-deterministic" aria-label="DPO will overfit when the preference is too deterministic">DPO will overfit when the preference is too deterministic</a></li></ul>
                </li>
                <li>
                    <a href="#summary" aria-label="Summary">Summary</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>It&rsquo;s well known that the state-of-the-art LLM models are trained with massive human quality feedback. This feedback is either coming from a massive rater pool, or can come from the end users implicitly (sometimes explicitly as well.. remember when ChatGPT <a href="https://snipboard.io/O1mE5u.jpg">presents</a> you 2 responses to choose from?).</p>
<p>However, I found that there are many subtleties for one to truly understand what&rsquo;s going on under the hood. This is a blog to capture my understanding of the RLHF algorithm, and how it evolves into rewardless model such as DPO and IPO.</p>
<h2 id="a-quick-recap-of-rlhf">A quick recap of RLHF<a hidden class="anchor" aria-hidden="true" href="#a-quick-recap-of-rlhf">#</a></h2>
<p>Suppose our reward is point-wise, meaning we get a single float number to denote the quality of the response based on the context, we try to optimize for the following objective in RLHF</p>
<p>$$
\begin{equation}
\begin{aligned}
J(\pi) &amp;= \mathbb{E}_{\substack{x \sim D \\ y \sim \pi(\cdot|x)}} \left[ r(x, y) \right] - \beta\mathbb{D}_\text{KL}(\pi ||\pi_\text{ref})
\end{aligned}
\end{equation}
$$</p>
<p>
The reward function \( r(x, y) \) usually isn't directly differentiable in relative to \(\pi\) itself (for example, in auto-regressive model where we sample each token with argmax). Thus the most common approach to deal with this case is to use actor critic method to find the best policy.
</p>
<p>Optimizing equation (1) usually consists of the following general steps, assuming we are using PPO</p>
<p><strong>1. Training the reward model</strong></p>
<p>
Suppose we have access of many \( (\text{prompt}, \text{likert score}) \) pairs, we can train the reward model \( r(x, y) \) with MSE loss and with a neural network. One good thing is that such neural network can be directly initialized with the underlying policy transformer network, and replace the output layers to be a shallow FFN.
</p>
<p><strong>2. Sample the policy, and estimate value</strong></p>
<p>
During this step, suppose we have access to many prompts that matches the distribution we want to optimize, say \( x \sim D \), we can thus run the policy \(T\) times and send the responses for reward scoring. This will results a distribution \( \text{M} \) with \(T\) episodes where each episode \( \text{M}^t \) consists of:
</p>
<p>$$
\text{M}^t = (\text{prompt x}^t, \text{response y}^t, \text{score r}^t, \text{value v}^t, \text{probability p}^t)
$$</p>
<p>Additionally, we also generate 2 more things:</p>
<ul>
<li><em>value</em>: This is generated by a <strong>value network</strong>, which is needed to stabilize the RL training to make it offset invariant. See <a href="https://arxiv.org/abs/1707.06347">PPO</a> paper for details. One good thing about it is that such value network can be initialized by the policy SFT&rsquo;ed network too, similar to our reward model.</li>
<li><em>probability</em>: This is the probability of the response sequence (of length n), equals to
$$
\prod_{i=1}^{n} p(y_i \mid y_1, y_2, \ldots, y_{i-1}, x)
$$
this will later be used to calculate the KL divergence.</li>
</ul>
<p><strong>3. Train with PPO</strong></p>
<p>
Following the common PPO training, we have the loss function across all the \(T\) episodes as
</p>
<p>$$
\begin{aligned}
L^{\text{PPO}}(\pi) = \mathbb{E}_{t \sim M} \Bigg[ &amp; \min \left( \rho_t(\pi) \hat{A}_t, \text{clip} \left( \rho_t(\pi), 1 - \epsilon, 1 + \epsilon \right) \hat{A}_t \right) \Bigg]
\end{aligned}
$$</p>
<p>where,
$$
\begin{aligned}
\rho_t(\pi) &amp;= \frac{\pi(y^t | x^t)}{\pi_{\text{ref}}(y^t | x^t)} \\
\hat{A}_t &amp;= J_t - v^t \\
J_t &amp;= r^t - \beta\mathbb{D}_{\text{KL}}(\pi || \pi_{\text{ref}})
\end{aligned}
$$</p>
<p>I think it&rsquo;s worth pointing out the calculation of the KL divergence (at least it took me a long time to figure out). The definition of KL divergence is</p>
<p>$$
D_{\text{KL}}(\pi \parallel \pi_{\text{ref}}) = \sum_{y} \pi(y|x) \log \left( \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} \right)
$$</p>
<p>comprehensively evaluate for all y is computationally intractable (like, you need to exhaust all possible human sentences). But recall that the meaning of KL divergence is measure how different 2 distribution is, and that can also be translated to how <strong>different 2 distribution assign probability to the same sequence sample from one of them</strong><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>Thus a noisy estimation of the KL divergence can be expressed as</p>
<p>$$
D_{\text{KL}}(\pi \parallel \pi_{\text{ref}}) = \mathbb{E}_{x \sim M}[\text{KL}(\pi(\cdot|x) || \pi_{\text{ref}}(\cdot|x))]
$$</p>
<p>where</p>
<p>$$
\mathbb{E}_{x \sim M}[\text{KL}(\pi(\cdot|x) || \pi_{\text{ref}}(\cdot|x))] = \mathbb{E}_{y \sim \pi(\cdot|x)}\Bigg[\log(\frac{\pi(y|x)}{\pi_{\text{ref}(y|x)}}) \Bigg]
$$</p>
<p>You can also check <a href="https://github.com/lucidrains/PaLM-rlhf-pytorch/blob/main/palm_rlhf_pytorch/ppo.py#L508-L509">this implementation</a> to gain a deeper understanding of it.</p>
<p><strong>4. Update value estimator</strong></p>
<p>Finally, once the model has been updated, the value function is fitting to the actual reward. You repeat 2-4 until the model is aligned with the preference.</p>
<h2 id="deal-with-pairwise-preference">Deal with pairwise preference<a hidden class="anchor" aria-hidden="true" href="#deal-with-pairwise-preference">#</a></h2>
<p>The previous section is assuming a point wise preference labeling, that is, each reward labeling is of pair
$$
(\text{prompt}, \text{likert score})
$$
However, human is not very good at rating absolutely, but very good as comparing things side by side. For example, it&rsquo;s hard to say how hot the weather is exactly in terms of Celsius, but it&rsquo;s quite easy to tell the the weather in the evening is more chill compared to the afternoon. The same intuition applies for LLM training, while it&rsquo;s hard to collect absolute likert score, it&rsquo;s relatively easy to collect a triplet of
$$
(\text{prompt}, \text{response}_w, \text{response}_l)
$$</p>
<p>
Where the 2 responses are 2 samples and we collect the preference score from a rater pool about which one is better (\(\text{response}_w\)) and which one is worse (\(\text{response}_l\)) in the context of the prompt.
</p>
<p>Rather than training the reward model to fit the absolute score, the reward model instead trained to fit a Bradley-Terry model <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>
Let the prompt be \(x\) and \( \text{response}_w, \text{response}_l \) be \(y_w, y_l\) respectively. The objective instead becomes
</p>
<p>$$
L(r) = -\mathbb{E}_{(x, y_w, y_l)}[\log(p(y_w&gt;y_l|x))]
$$
where
$$
p(y_w&gt;y_l|x) = \sigma(r(x,y_w) - r(x, y_l))
$$</p>
<p>See this <a href="https://github.com/OpenLLMAI/OpenRLHF/blob/main/openrlhf/models/loss.py#L80-L92">implementation</a> to get a deeper understanding of the pairwise reward model training. Once the reward model is trained, we use PPO with the same objective as in equation (1) to perform RLHF.</p>
<h2 id="dpo-avoid-the-reinforcement-learning-at-all">DPO: Avoid the reinforcement learning at all<a hidden class="anchor" aria-hidden="true" href="#dpo-avoid-the-reinforcement-learning-at-all">#</a></h2>
<p>Let&rsquo;s recap, in RLHF (with PPO), we need to bring up 3 models to do different things. We need our policy model to create episodes and update, a reward model to score the generated response and a value model to estimate the value in order to stabilize the training. Although theoretically a lot of parameters can be shared across them <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>, it is still relatively resource intensive compared to instruction fine tuning. In addition, it requires an additional step to train the reward model, which leaves more room for error.</p>
<p><img loading="lazy" src="/rlhf/model_training.png" alt="3 different model copies"  />
</p>
<p>Thus, we are interested in a reward free method which can be more resource/time efficient method that can avoid the above issue. And guess what, DPO does exact that with a quite beautiful mathematic trick.</p>
<p>Recall that our objective, even though non differentiable, is
$$
\begin{aligned}
J(\pi) &amp;= \mathbb{E}_{\substack{x \sim D \\ y \sim \pi(\cdot|x)}} \left[ r(x, y) \right] - \beta\mathbb{D}_\text{KL}(\pi ||\pi_\text{ref})
\end{aligned}
$$
Let&rsquo;s see if we can have an analytic solution to the optimal policy
$$
\begin{aligned}
\max_\pi J &amp;= \max_\pi\mathbb{E}_{\substack{x \sim D \\ y \sim \pi(\cdot|x)}} \left[ r(x, y) \right] - \beta\mathbb{D}_\text{KL}(\pi ||\pi_\text{ref}) \\
&amp;= \max_\pi\mathbb{E}_{\substack{x \sim D \\ y \sim \pi(\cdot|x)}} \left[ r(x, y) \right] - \beta\mathbb{E}_{\substack{x \sim D \\ y \sim \pi(\cdot|x)}}\Bigg[\log(\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)})\Bigg] \\
&amp;= \max_\pi\mathbb{E}_{\substack{x \sim D \\ y \sim \pi(\cdot|x)}} \Bigg[ r(x, y) - \beta\log(\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)})\Bigg] \\
&amp;= \min_\pi\mathbb{E}_{\substack{x \sim D \\ y \sim \pi(\cdot|x)}} \Bigg[ \beta\log(\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}) - r(x, y) \Bigg] \\
&amp;= \min_\pi\mathbb{E}_{\substack{x \sim D \\ y \sim \pi(\cdot|x)}} \Bigg[ \beta\log(\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)\cdot\exp(\frac{1}{\beta}r(x, y))}) \Bigg] \\
&amp;= \beta\min_\pi\mathbb{E}_{\substack{x \sim D \\ y \sim \pi(\cdot|x)}} \Bigg[ \log{\frac{\pi(y|x)}{\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\cdot\exp(\frac{1}{\beta}r(x, y))}} - \log{Z(x)}\Bigg] \\
\end{aligned}
$$</p>
<p>
let \(Z(x) = \sum_{\text{all of } y \sim \pi(\cdot|x)}{\pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x, y))} \), we can see that the denominator in the above equation is a valid distribution (probability adds up to 1). Thus, the objective function becomes 
</p>
$$
\begin{equation}
\begin{aligned}
\max_\pi J &= \beta\min_\pi\mathbb{E}_{x \sim D } [\mathbb{D}_\text{KL}(\pi(y|x) ||\pi_*(y|x)) - \log{Z(x)}]
\end{aligned}
\end{equation}
$$
where
$$
\begin{equation}
\pi_*(y|x) = \frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\cdot\exp(\frac{1}{\beta}r(x, y))
\end{equation}
$$
<p>
because \(Z(x)\) is exactly irrelevant to \(\pi\), we know that solution to equation (2) is \(\pi = \pi_*\). Thus equation (3) is our optimal policy model that perfectly aligns with the reward model.
</p>
<p>
Up until know, our conclusion in (3) doesn't help so much. That's because we have a nasty term \(Z(x)\) which requires up to calculate the probability of all possible y given x which is impractical in practice.
</p>
<p>But here comes the fun part: <strong>With some mathematic tricks, we can cancel out the incomputable Z and get a computable loss function!:</strong></p>
<p>
To see how, let's assume our reward model \(r^*\) fits the human preference with a Bradley-Terry model. That is, we have 
</p>
$$
\begin{equation}
p(y_w>y_l|x) = \sigma(r(x, y_w) - r(x, y_l))
\end{equation}
$$
<p>
Because we have \(\pi_*(y|x) = \frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\cdot\exp(\frac{1}{\beta}r(x, y))\), we can reparametrize the optimal reward as 
</p>
$$
\begin{equation}
r(x, y) = \beta\log{\frac{\pi^*(y|x)}{\pi_{\text{ref}}(y|x)}} + \beta\log{Z(x)}
\end{equation}
$$
<p>If we substitute (5) into (4), surprisingly we canceled out the term Z! And the Bradley-Terry model can be expressed as
$$
\begin{equation}
p(y_w&gt;y_l|x) = \sigma\Bigg(\beta\log{\frac{\pi^*(y_w|x)}{\pi_{\text{ref}}(y_w|x)}} -  \beta\log{\frac{\pi^{\ast}(y_l|x)}{\pi_{\text{ref}}(y_l|x)}}\Bigg)
\end{equation}
$$</p>
<p>I just want to stop here and appreciate the beauty of the above derivation. Let&rsquo;s think about what this final equation (6) actually implies:</p>
<ul>
<li>Finding the perfect reward model that fits the pair wise preference in (4) is mathematically equivalent to finding a perfect policy model that fits the same pair wise preference.</li>
<li>Any reward model has a corresponding policy model that captures the same learned preference (equation (3))</li>
</ul>
<p>Thus, our final objective becomes
$$
\begin{equation}
L_{\text{DPO}}(\pi;\pi_{\text{ref}})=-\mathbb{E}_{(x,y_w,y_l) \sim D}\Bigg[ \log{\sigma\Bigg(\beta\log{\frac{\pi^*(y_w|x)}{\pi_{\text{ref}}(y_w|x)}} -  \beta\log{\frac{\pi^{\ast}(y_l|x)}{\pi_{\text{ref}}(y_l|x)}}\Bigg)} \Bigg]
\end{equation}
$$</p>
<p>Note that the above objective (7) is differentiable, and thus we don&rsquo;t need reinforcement learning to learn the objective, but instead can just <strong>train the policy model with back propagation!</strong> This thus greatly simplifies our training process.</p>
<p>We suggest you to also look at the original DPO paper where it analyze the property of its gradient to build more intuition. But to sum up, the policy model together with the original reference model (SFT model) encodes implicitly the preference in the reward model training dataset. Such preference doesn&rsquo;t require training a RM as a proxy but instead can directly train the policy model to learn. Due to such simplicity, in fact most of the open source large language model use some version of DPO to train the model.</p>
<h2 id="limitation-of-dpo">Limitation of DPO<a hidden class="anchor" aria-hidden="true" href="#limitation-of-dpo">#</a></h2>
<p>But is DPO the perfect method to train LLM? Why we never heard that OpenAI is using DPO to perform GPT post-training?</p>
<p>Many works<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup><sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup><sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> analyze the problem, and shows that DPO has the following theoretical limitations:</p>
<h3 id="dpo-will-assign-high-value-to-out-of-distribution-samples-4">DPO will assign high value to out-of-distribution samples <sup id="fnref1:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup><a hidden class="anchor" aria-hidden="true" href="#dpo-will-assign-high-value-to-out-of-distribution-samples-4">#</a></h3>
<p>Suppose we have only 3 actions listed in the table below</p>
<table>
<thead>
<tr>
<th><strong>Action</strong></th>
<th><strong>y<sub>1</sub></strong></th>
<th><strong>y<sub>2</sub></strong></th>
<th><strong>y<sub>3</sub></strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>π<sub>ref</sub></td>
<td>0.5</td>
<td>0.5</td>
<td>0</td>
</tr>
<tr>
<td>D<sub>pref</sub></td>
<td>{(y<sub>w</sub> = y<sub>1</sub>, y<sub>l</sub> = y<sub>2</sub>)}</td>
<td></td>
<td></td>
</tr>
<tr>
<td>π<sub>DPO</sub></td>
<td>0.1</td>
<td>0.0</td>
<td>0.9</td>
</tr>
<tr>
<td>π<sub>PPO</sub></td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>We can see that DPO can learn the solution of 0.1, 0, 0.9. This is because the reward function</p>
<p>$$
\begin{aligned}
L_{\text{DPO}}(\pi;\pi_{\text{ref}})&amp; =-\mathbb{E}_{(x,y_w,y_l) \sim D}\Bigg[ \log{\sigma\Bigg(\beta\log{\frac{\pi^*(y_w|x)}{\pi_{\text{ref}}(y_w|x)}} -  \beta\log{\frac{\pi^{\ast}(y_l|x)}{\pi_{\text{ref}}(y_l|x)}}\Bigg)} \Bigg] \\
&amp;= -\log{\sigma\Bigg(\beta(\log(\frac{a}{0.5})-\log{\frac{b}{0.5}})\Bigg)} \\
&amp;= \log\Bigg(1+(\frac{a}{b})^\beta\Bigg)
\end{aligned}
$$</p>
<p>
Thus as long as \(a=0\), it is an optimal policy from the view of DPO. However, such policy won't be an optimal one for PPO training, as the KL divergence will ensure the model to stay close to the reference model.
</p>
<p>This is also discovered in the Nvidia Nemotron<sup id="fnref1:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> training, where in the tech report, they reported that</p>
<blockquote>
<p>While the policy learns to differentiate chosen and rejected responses, we observe both chosen and rejected responses’ likelihoods drop consistently with their gap increasing, even if chosen responses are high-quality.</p>
</blockquote>
<p>In addition, RLHF with PPO can leverage prompt only date, and such dataset act as a good regularizer of the model training to avoid it bias toward OOD samples.</p>
<h3 id="dpo-will-overfit-when-the-preference-is-too-deterministic">DPO will overfit when the preference is too deterministic<a hidden class="anchor" aria-hidden="true" href="#dpo-will-overfit-when-the-preference-is-too-deterministic">#</a></h3>
<p>
In DeepMind's analysis, under the assumption that \(p^\ast\) fits a Bradley-Terry model, we can prove that for the learning objective below
</p>
<p>$$
\max_\pi\mathbb{E}_{\substack{x \sim D \\ y \sim \pi(\cdot|x) \\ y&rsquo; \sim \mu(\cdot|x)}}[\Psi(p^\ast(y&gt;y&rsquo;|x))] - \tau\mathbb{D}_{\text{KL}}(\pi || \pi_{\text{ref}})
$$</p>
<p>it is exactly the same as the RLHF objective in (1), and thus equivalent to the DPO learning objective in (7), when
$$
\Psi(q) = \log(\frac{q}{1-q})
$$</p>
<p>
It's actually quite easy to prove this proposition and we leave that to the reader. But looking at the shape of this function, we can see that this function has a very large gradient near the point of 0 and 1. When training with this objective, a small increase in the preference near 0 or 1 will just be as incentivized as a large increase near 0.5. Thus, when the ground truth label is very deterministic (such as machine based feedback), thus no matter how large the regularization term \(\tau\) is, the model will eventually deviate away from the reference and thus overfit drastically.
</p>
<p><img loading="lazy" src="/rlhf/log_psi.png" alt="psi_function"  />
</p>
<p>This is also demonstrated in the experiment of IPO, where no matter how large the regularizer is, the policy will eventually deviate away from the original one. However, this won&rsquo;t happen in RLHF not only because a RM is trained to proxy the raw preference and serves as a regularizer, it utilize PPO which only moves the policy in a trust region. This might be why DeepSeek V2 Coder is using an RM to regularize the execution feedback instead of sending it directly for learning.</p>
<p><img loading="lazy" src="/rlhf/ipo.jpeg" alt="ipo"  />
</p>
<h2 id="summary">Summary<a hidden class="anchor" aria-hidden="true" href="#summary">#</a></h2>
<p>Okay, you made it to the end, congrats :) Before you go, let&rsquo;s just recap what we have learned:</p>
<ul>
<li>RLHF with PPO is the standard way of aligning the post SFT model towards annotated human preference. Pairwise preference is fitted with a Bradley-Terry model.</li>
<li>DPO utilize a reparametrization trick, and prove that theoretically the policy model captures the preference and can be directly learned via back propagation.</li>
<li>While DPO is simpler and more computational efficient, it suffers from regularization problem for both OOD and deterministic feedback (such as 0/1), thus while it generally performs well in more subjective tasks where a multi-scale likert score is used, it should be used with caution when the feedback is too deterministic or preference dataset is too small.</li>
</ul>
<p>I hope you like this post and please don&rsquo;t hesitate to give your suggestion (scroll all the way up and click &ldquo;Suggest Changes&rdquo;). Thanks!</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>See this <a href="https://www.youtube.com/watch?v=SxGYPqCgJWM">video</a> for a very good explanation with coin toss.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://web.stanford.edu/class/archive/stats/stats200/stats200.1172/Lecture24.pdf">https://web.stanford.edu/class/archive/stats/stats200/stats200.1172/Lecture24.pdf</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Google Sparrow: <a href="https://arxiv.org/abs/2209.14375">https://arxiv.org/abs/2209.14375</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>DPO will increase the likelihood of sample that&rsquo;s OOD of preference pairs <a href="https://arxiv.org/abs/2404.10719">https://arxiv.org/abs/2404.10719</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Nvidia NemoTron Tech Report: <a href="https://arxiv.org/html/2406.11704v1">https://arxiv.org/html/2406.11704v1</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Google IPO: <a href="https://arxiv.org/abs/2310.12036">https://arxiv.org/abs/2310.12036</a>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/llm/">LLM</a></li>
      <li><a href="http://localhost:1313/tags/reinforcement-learning/">Reinforcement Learning</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="http://localhost:1313/posts/lesson_i_learned/">
    <span class="title">Next »</span>
    <br>
    <span>Some lessons I learned the hard way</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share From RLHF to DPO on x"
            href="https://x.com/intent/tweet/?text=From%20RLHF%20to%20DPO&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2frlhf_to_ipo%2f&amp;hashtags=LLM%2cReinforcementLearning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share From RLHF to DPO on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2frlhf_to_ipo%2f&amp;title=From%20RLHF%20to%20DPO&amp;summary=From%20RLHF%20to%20DPO&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2frlhf_to_ipo%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share From RLHF to DPO on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2frlhf_to_ipo%2f&title=From%20RLHF%20to%20DPO">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share From RLHF to DPO on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2frlhf_to_ipo%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share From RLHF to DPO on whatsapp"
            href="https://api.whatsapp.com/send?text=From%20RLHF%20to%20DPO%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2frlhf_to_ipo%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share From RLHF to DPO on telegram"
            href="https://telegram.me/share/url?text=From%20RLHF%20to%20DPO&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2frlhf_to_ipo%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share From RLHF to DPO on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=From%20RLHF%20to%20DPO&u=http%3a%2f%2flocalhost%3a1313%2fposts%2frlhf_to_ipo%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="http://localhost:1313/">Weilun&#39;s Homepage</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
