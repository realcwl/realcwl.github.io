---
author: ["Weilun Chen"]
title: "From RLHF to DPO and IPO"
date: "2024-07-04"
description: "How does RLHF works under the hood, and how can we get rid of it? (Or can we?)"
tags: ["machine learning", "LLM", "Reinforcement Learning"]
math: true
ShowToc: true
---

I think by now every one should already understand that the state-of-the-art LLM models are trained with massive human quality feedback. This feedback is either coming from a massive rater pool, or can come from the end users implicitly (sometimes explicitly as well.. remember when ChatGPT [presents](https://snipboard.io/O1mE5u.jpg) you 2 responses to choose from?). This [video](https://youtu.be/zjkBMFhNj_g?si=LaoFxRBqdKuKRdJD&t=1262) talks about RLHF very briefly and you can checkout for a refresher. The content below requires you to have a rough understanding of this algorithm.

## A quick recap of RLHF

Suppose our reward is point-wise, meaning we get a single float number to denote the quality of the response based on the context, we try to optimize for the following objective in RLHF

$$
\begin{equation}
\begin{aligned}
J(\pi) &= \mathbb{E}\_{\substack{x \sim D \\\ y \sim \pi(\cdot|x)}} \left[ r(x, y) \right] - \beta\mathbb{D}\_\text{KL}(\pi ||\pi_\text{ref})
\end{aligned}
\end{equation}
$$

<p>
The reward function \( r(x, y) \) usually isn't directly differentiable in relative to \(\pi\) itself (for example, in auto-regressive model where we sample each token with argmax). Thus the most common approach to deal with this case is to use actor critic method to find the best policy.
</p>

Optimizing equation (1) usually consists of the following general steps, assuming we are using PPO

**1. Training the reward model**
<p>
Suppose we have access of many \( (\text{prompt}, \text{score}) \) pairs, we can train the reward model \( r(x, y) \) with MSE loss and with a neural network. One good thing is that such neural network can be directly initialized with the underlying policy transformer network, and replace the output layers to be a shallow FFN.
</p>

**2. Sample the policy, and estimate value**
<p>
During this step, suppose we have access to many prompts that matches the distribution we want to optimize, say \( x \sim D \), we can thus run the policy \(T\) times and send the responses for reward scoring. This will results in \(T\) episodes where each episode \( \text{M} \) consists of:
</p>

$$
\text{M}^t = (\text{prompt x}^t, \text{response y}^t, \text{score r}^t, \text{value v}^t, \text{probability p}^t)
$$

Additionally, we also generate 2 more things:

- *value*: This is generated by a **value network**, which is needed to stabilize the RL training to make it offset invariant. See [PPO](https://arxiv.org/abs/1707.06347) paper for details. One good thing about it is that such value network can be initialized by the policy SFT'ed network too, similar to our reward model.
- *probability*: This is the probability of the response sequence (of length n), equals to
  $$
  \prod_{i=1}^{n} p(y_i \mid y_1, y_2, \ldots, y_{i-1}, x)
  $$
  this will later be used to calculate the KL divergence.

**3. Train with PPO**
<p>
Following the common PPO training, we have the loss function across all the \(T\) episodes as
</p>

$$
\begin{aligned}
L^{\text{PPO}}(\pi) = \mathbb{E}_t \Bigg[ & \min \left( \rho_t(\pi) \hat{A}_t, \text{clip} \left( \rho_t(\pi), 1 - \epsilon, 1 + \epsilon \right) \hat{A}_t \right) \Bigg]
\end{aligned}
$$

where,
$$
\begin{aligned}
\rho_t(\pi) &= \frac{\pi(y^t | x^t)}{\pi_{\text{ref}}(y^t | x^t)} \\\
\hat{A}_t &= J_t - v^t \\\
J_t &= \frac{1}{T}\sum\_{i=1}^{T}r^t - \beta\mathbb{D}\_{\text{KL}}(\pi || \pi\_{\text{ref}})
\end{aligned}
$$

For 